{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVqrsVWh0kiC"
   },
   "source": [
    "# Named Entity Recognition Assignment\n",
    "NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. In this assignment, you will train a named entity recognition system and test it on a test data. \\\n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WR6a6DkN0d-3",
    "ExecuteTime": {
     "end_time": "2024-12-19T08:29:06.667298Z",
     "start_time": "2024-12-19T08:28:55.486398Z"
    }
   },
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils_2 import load_data_file, get_vocab, get_tags\n",
    "import random as rnd"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xAbdoMo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ulSik2Sv1p1G",
    "ExecuteTime": {
     "end_time": "2024-12-19T08:29:06.708766Z",
     "start_time": "2024-12-19T08:29:06.668310Z"
    }
   },
   "source": [
    "vocab = get_vocab(['processed_input/train_vocab.txt', 'processed_input/dev_vocab.txt', 'processed_input/test_vocab.txt'])\n",
    "\n",
    "input_name = \"pizza_orders\"\n",
    "labels_name = \"pizza_orders_labels\"\n",
    "\n",
    "tags = get_tags(f'processed_input/train_{input_name}_tags.txt')\n",
    "\n",
    "print(len(vocab))\n",
    "print(tags)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n",
      "{'TOPPING_S': 0, 'COMPLEX_TOPPING_S': 1, 'NOT_STYLE_S': 2, 'STYLE': 3, 'NUMBER': 4, 'TOPPING': 5, 'SIZE': 6, 'NONE': 7, 'NOT_TOPPING': 8, 'NOT_TOPPING_S': 9, 'NUMBER_S': 10, 'NOT_COMPLEX_TOPPING_S': 11, 'STYLE_S': 12, 'NOT_COMPLEX_TOPPING': 13, 'COMPLEX_TOPPING': 14, 'SIZE_S': 15, 'NOT_STYLE': 16}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PagjN4rl22Fr",
    "outputId": "f65e741a-74e1-45f6-8361-58a0ce4cc818",
    "ExecuteTime": {
     "end_time": "2024-12-19T08:33:00.935098Z",
     "start_time": "2024-12-19T08:29:06.709780Z"
    }
   },
   "source": [
    "t_sentences, t_labels, t_size = load_data_file(vocab, tags, f'processed_input/train_{input_name}.txt', f'processed_input/train_{labels_name}.txt')\n",
    "\n",
    "dev_sentences, dev_labels, dev_size =  load_data_file(vocab, tags, f'processed_input/dev_{input_name}.txt', f'processed_input/dev_{labels_name}.txt')\n",
    "\n",
    "test_sentences, test_labels, test_size =  load_data_file(vocab, tags, f'processed_input/test_{input_name}.txt', f'processed_input/test_{labels_name}.txt')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWLR2Oxp28K6",
    "outputId": "3b13ca97-f85c-42ca-b84a-bcca9868137c",
    "ExecuteTime": {
     "end_time": "2024-12-19T08:33:00.939702Z",
     "start_time": "2024-12-19T08:33:00.936108Z"
    }
   },
   "source": [
    "print('The number of outputs is tag_map', len(tags))\n",
    "print('The vocab size is', len(vocab))\n",
    "print('The training size is', t_size)\n",
    "print('The validation size is', dev_size)\n",
    "print('The testing size is', test_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is tag_map 17\n",
      "The vocab size is 448\n",
      "The training size is 2493488\n",
      "The validation size is 367\n",
      "The testing size is 1420\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xHeJcz1JuhYa",
    "ExecuteTime": {
     "end_time": "2024-12-19T08:33:00.961682Z",
     "start_time": "2024-12-19T08:33:00.939702Z"
    }
   },
   "source": "from ner import NER, NERDataset",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-yvaq8i2CCLD",
    "ExecuteTime": {
     "end_time": "2024-12-19T08:33:00.967512Z",
     "start_time": "2024-12-19T08:33:00.961682Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def train(model, train_dataset, batch_size=512, epochs=10, learning_rate=0.01, skip_prop=0.0):\n",
    "  model.train()   # switch to train mode\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.cuda(device)\n",
    "    pass\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "  \n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "      if skip_prop > random.random():\n",
    "        continue    # skip this batch\n",
    "      train_input = train_input.to(device)\n",
    "      train_label = train_label.to(device)\n",
    "      output = model.forward(train_input)\n",
    "      batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n",
    "      total_loss_train += batch_loss\n",
    "      acc = (torch.argmax(output, dim=-1) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "      optimizer.zero_grad()\n",
    "      batch_loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "  \n",
    "    sample_count = len(train_dataset)\n",
    "    seq_length = train_dataset[0][0].shape[0]\n",
    "    epoch_acc = total_acc_train / (sample_count * seq_length)\n",
    "  \n",
    "  \n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3BI7_ANkLf7G",
    "ExecuteTime": {
     "end_time": "2024-12-19T08:33:16.530821Z",
     "start_time": "2024-12-19T08:33:00.968523Z"
    }
   },
   "source": [
    "train_dataset = NERDataset(t_sentences, t_labels, vocab['<pad>'], tags[\"NONE\"])\n",
    "val_dataset = NERDataset(dev_sentences, dev_labels, vocab['<pad>'], tags[\"NONE\"])\n",
    "test_dataset = NERDataset(test_sentences, test_labels, vocab['<pad>'], tags[\"NONE\"])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:33:16.747259Z",
     "start_time": "2024-12-19T08:33:16.531433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NER(embedding_dim=95, hidden_size=1200, n_classes=len(tags), vocab_size=len(vocab), num_layers=2, dropout=0.5)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER(\n",
      "  (embedding): Embedding(448, 95)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (lstm): LSTM(95, 1200, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (linear): Linear(in_features=2400, out_features=17, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T13:26:30.088336Z",
     "start_time": "2024-12-19T13:00:43.567611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = model.to(torch.device(\"cuda:0\"))\n",
    "train(model, train_dataset, epochs=10, batch_size=64, skip_prop=0.9)\n",
    "# train(model, val_dataset, epochs=15, batch_size=32)\n",
    "# train(model, test_dataset, epochs=15, batch_size=32)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38961/38961 [05:49<00:00, 111.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 2.127582411048934e-05         | Train Accuracy: 0.09961727355033438\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38961/38961 [05:50<00:00, 111.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 1.8349768652115017e-05         | Train Accuracy: 0.09977566708619756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38961/38961 [05:46<00:00, 112.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 1.884028642962221e-05         | Train Accuracy: 0.09850065068538147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38961/38961 [05:47<00:00, 112.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 2.108085209329147e-05         | Train Accuracy: 0.09933889127733253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 16871/38961 [02:31<03:18, 111.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_prop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.9\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# train(model, val_dataset, epochs=15, batch_size=32)\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# train(model, test_dataset, epochs=15, batch_size=32)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[6], line 23\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_dataset, batch_size, epochs, learning_rate, skip_prop)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m skip_prop \u001B[38;5;241m>\u001B[39m random\u001B[38;5;241m.\u001B[39mrandom():\n\u001B[0;32m     22\u001B[0m   \u001B[38;5;28;01mcontinue\u001B[39;00m    \u001B[38;5;66;03m# skip this batch\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m train_input \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m train_label \u001B[38;5;241m=\u001B[39m train_label\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     25\u001B[0m output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(train_input)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T13:29:15.765608Z",
     "start_time": "2024-12-19T13:28:59.340078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train(model, val_dataset, epochs=5, batch_size=32)\n",
    "train(model, test_dataset, epochs=5, batch_size=32)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.02269425429403782         | Train Accuracy: 0.9716255868544601\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.01942363940179348         | Train Accuracy: 0.9728579812206573\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.016794899478554726         | Train Accuracy: 0.9742957746478873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.01656949706375599         | Train Accuracy: 0.9762910798122065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 14.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.014965523965656757         | Train Accuracy: 0.9770246478873239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gz5mxUAJM1xS",
    "ExecuteTime": {
     "end_time": "2024-12-19T10:03:59.001358Z",
     "start_time": "2024-12-19T10:03:58.990467Z"
    }
   },
   "source": [
    "def evaluate(model, test_dataset, batch_size=64):\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size,shuffle=False)\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for test_input, test_label in tqdm(test_dataloader):\n",
    "      test_input = test_input.to(device)\n",
    "      test_label = test_label.to(device)\n",
    "      output = model.forward(test_input)\n",
    "\n",
    "      # Check if entire sequence matches by comparing all positions\n",
    "      sequence_matches = (torch.argmax(output, dim=-1) == test_label).all(dim=-1)\n",
    "      acc = sequence_matches.sum().item()\n",
    "      total_acc_test += acc\n",
    "    \n",
    "    total_acc_test /= len(test_dataset)\n",
    "  print(f'\\nTest Accuracy: {total_acc_test}')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FD8JNcHWmMY",
    "outputId": "b4916766-dd57-4716-db7f-90c6d46655fa",
    "ExecuteTime": {
     "end_time": "2024-12-19T13:29:53.209611Z",
     "start_time": "2024-12-19T13:29:51.845182Z"
    }
   },
   "source": [
    "model.eval()\n",
    "evaluate(model, test_dataset)\n",
    "evaluate(model, val_dataset)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 20.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.8380281690140845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 31.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.8583106267029973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T13:34:13.883455Z",
     "start_time": "2024-12-19T13:34:13.196369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inv_order_tags = {}\n",
    "for tag, value in tags.items():\n",
    "  inv_order_tags[value] = tag\n",
    "\n",
    "from utils_2 import tokenize, preprocess_tokens, project_tokens \n",
    "def test_sample(sample, model):\n",
    "  s = tokenize(sample)\n",
    "  s = preprocess_tokens(s, 0)\n",
    "  print(s)\n",
    "  s = project_tokens(s, vocab)\n",
    "  x_tensor = torch.tensor(s)\n",
    "  device = torch.device(\"cuda:0\")\n",
    "  with torch.no_grad():\n",
    "    output = model.forward(x_tensor.to(device))\n",
    "    output = torch.argmax(output, dim=-1).to(\"cpu\")\n",
    "    print([inv_order_tags[x.item()] for x in output])\n",
    "\n",
    "model.eval()\n",
    "test_sample(\"two large pizzas and peperoni with extra olives and chicken and extra osama\", model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<num>', 'larg', 'pizza', 'and', 'peperoni', 'with', 'extra', 'oliv', 'and', 'chicken', 'and', 'extra', 'osama']\n",
      "Word: osama not in vocab\n",
      "isPersonalPronoun: False\n",
      "isNumber: False\n",
      "isTopping: False\n",
      "isQuantity: False\n",
      "['NUMBER_S', 'SIZE_S', 'NONE', 'NONE', 'TOPPING_S', 'NONE', 'COMPLEX_TOPPING_S', 'COMPLEX_TOPPING', 'NONE', 'TOPPING_S', 'NONE', 'COMPLEX_TOPPING_S', 'NONE']\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T13:30:20.159495Z",
     "start_time": "2024-12-19T13:30:18.964446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model_io import save_model_state, load_model_state\n",
    "\n",
    "save_model_state(model, \"models/pizza_order_e95_h1200_l2_d0.5_r2_x83.8pth\")\n"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T06:54:37.859448Z",
     "start_time": "2024-12-19T06:54:37.539603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_load_test = NER(\n",
    "  embedding_dim=95, hidden_size=1200, n_classes=len(tags), vocab_size=len(vocab), num_layers=2, dropout=0.5\n",
    ")\n",
    "\n",
    "load_model_state(model_load_test, \"models/order_boundary_e95_h1200_l2_d0.5_r1_x91.4.pth\", torch.device(\"cuda:0\"))\n",
    "test_sample(\"two large pizzas with peperoni\", model_load_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pron>', 'would', 'like', '<num>', 'larg', 'pizza', 'with', 'everyth', 'and', '<num>', 'larg', 'pizza', 'with', 'mushroom', 'and', 'extra', 'chees', 'and', '<num>', 'larg', 'coke']\n",
      "['NONE', 'NONE', 'NONE', 'PIZZAORDER_S', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'NONE', 'PIZZAORDER_S', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'NONE', 'DRINKORDER_S', 'DRINKORDER', 'DRINKORDER']\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
