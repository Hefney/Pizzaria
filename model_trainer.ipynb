{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVqrsVWh0kiC"
   },
   "source": [
    "# Named Entity Recognition Assignment\n",
    "NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. In this assignment, you will train a named entity recognition system and test it on a test data. \\\n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WR6a6DkN0d-3",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:15:12.146655Z",
     "start_time": "2024-12-15T13:15:09.194588Z"
    }
   },
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils_2 import get_params, get_vocab, get_tags\n",
    "import random as rnd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_44BK5K82YwF"
   },
   "source": [
    "# Importing and discovering the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ulSik2Sv1p1G",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:15:51.515475Z",
     "start_time": "2024-12-15T13:15:41.780984Z"
    }
   },
   "source": [
    "vocab = get_vocab('processed_input/train_vocab.txt')\n",
    "tags = get_tags('processed_input/train_pizza_orders_tags.txt')\n",
    "\n",
    "input_name = \"pizza_orders\"\n",
    "labels_name = \"pizza_orders_labels\"\n",
    "\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tags, f'processed_input/train_{input_name}.txt', f'processed_input/train_{labels_name}.txt')\n",
    "\n",
    "dev_sentences, dev_labels, dev_size =  get_params(vocab, tags, f'processed_input/dev_{input_name}.txt', f'processed_input/dev_{labels_name}.txt')\n",
    "\n",
    "test_sentences, test_labels, test_size =  get_params(vocab, tags, f'processed_input/test_{input_name}.txt', f'processed_input/test_{labels_name}.txt')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PagjN4rl22Fr",
    "outputId": "f65e741a-74e1-45f6-8361-58a0ce4cc818",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:15:56.006360Z",
     "start_time": "2024-12-15T13:15:56.002506Z"
    }
   },
   "source": [
    "# The possible tags\n",
    "print(tags)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TOPPING_S': 0, 'COMPLEX_TOPPING_S': 1, 'NOT_STYLE_S': 2, 'STYLE': 3, 'NUMBER': 4, 'TOPPING': 5, 'SIZE': 6, 'NONE': 7, 'NOT_TOPPING': 8, 'NOT_TOPPING_S': 9, 'NUMBER_S': 10, 'NOT_COMPLEX_TOPPING_S': 11, 'STYLE_S': 12, 'NOT_COMPLEX_TOPPING': 13, 'COMPLEX_TOPPING': 14, 'SIZE_S': 15, 'NOT_STYLE': 16}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWLR2Oxp28K6",
    "outputId": "3b13ca97-f85c-42ca-b84a-bcca9868137c",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:16:00.465064Z",
     "start_time": "2024-12-15T13:16:00.461209Z"
    }
   },
   "source": [
    "# Exploring information about the data\n",
    "print('The number of outputs is tag_map', len(tags))\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "g_vocab_size = len(vocab)\n",
    "print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
    "print('The vocab size is', len(vocab))\n",
    "print('The training size is', t_size)\n",
    "print('The validation size is', dev_size)\n",
    "print('An example of the first sentence is', t_sentences[0])\n",
    "print('An example of its corresponding label is', t_labels[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is tag_map 17\n",
      "Num of vocabulary words: 307\n",
      "The vocab size is 307\n",
      "The training size is 2493488\n",
      "The validation size is 367\n",
      "An example of the first sentence is [178, 86, 146, 131, 150]\n",
      "An example of its corresponding label is [10, 15, 0, 5, 5]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "29iM0u4-4YOV",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:16:01.683561Z",
     "start_time": "2024-12-15T13:16:01.678262Z"
    }
   },
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, pad, tag_none):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    x_max = max(x, key= lambda z: len(z))\n",
    "    x = [sentence + [pad] * (len(x_max) - len(sentence)) for sentence in x]\n",
    "    y_max = max(y, key= lambda z: len(z))\n",
    "    y = [sentence + [tag_none] * (len(y_max) - len(sentence)) for sentence in y]\n",
    "    self.x_tensor = torch.tensor(x)\n",
    "    self.y_tensor = torch.tensor(y)\n",
    "    \n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    return self.x_tensor.shape[0]\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    return self.x_tensor[idx], self.y_tensor[idx]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sz-saCtRs7Pz",
    "outputId": "03e6cdf1-7785-4725-d4ad-fb085c70e1c5",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:16:02.330952Z",
     "start_time": "2024-12-15T13:16:02.318106Z"
    }
   },
   "source": [
    "batch_size = 5\n",
    "mini_sentences = t_sentences[0: 8]\n",
    "mini_labels = t_labels[0: 8]\n",
    "mini_dataset = NERDataset(mini_sentences, mini_labels, vocab['<PAD>'], tags[\"NONE\"])\n",
    "dummy_dataloader = torch.utils.data.DataLoader(mini_dataset, batch_size=5)\n",
    "dg = iter(dummy_dataloader)\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0][:])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 13]) torch.Size([5, 13]) torch.Size([3, 13]) torch.Size([3, 13])\n",
      "tensor([178,  86, 146, 131, 150, 306, 306, 306, 306, 306, 306, 306, 306]) \n",
      " tensor([10, 15,  0,  5,  5,  7,  7,  7,  7,  7,  7,  7,  7])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQB6O7I7FbUh"
   },
   "source": [
    "# NER\n",
    "The class that implementss the pytorch model for NER"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xHeJcz1JuhYa",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:16:04.096522Z",
     "start_time": "2024-12-15T13:16:04.091988Z"
    }
   },
   "source": [
    "class NER(nn.Module):\n",
    "  def __init__(self, vocab_size=len(vocab), embedding_dim=50, hidden_size=50, n_classes=len(tags)):\n",
    "    \"\"\"\n",
    "    The constructor of our NER model\n",
    "    Inputs:\n",
    "    - vacab_size: the number of unique words\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes (tags)\n",
    "    \"\"\"\n",
    "    super(NER, self).__init__()\n",
    "    ####################### TODO: Create the layers of your model #######################################\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "    self.dropout = nn.Dropout(p=0.25)\n",
    "    self.linear = nn.Linear(hidden_size * 2, n_classes) # x2 cuz bi-directional\n",
    "    #####################################################################################################\n",
    "\n",
    "  def forward(self, sentences):\n",
    "    \"\"\"\n",
    "    This function does the forward pass of our model\n",
    "    Inputs:\n",
    "    - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "    Returns:\n",
    "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    ######################### TODO: implement the forward pass ####################################\n",
    "    embedding = self.embedding(sentences)\n",
    "    lstm, _ = self.lstm(embedding)\n",
    "    dropout = self.dropout(lstm)\n",
    "    final_output = self.linear(dropout)\n",
    "    ###############################################################################################\n",
    "    return final_output"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLHx_oHpFlSX"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-yvaq8i2CCLD",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:16:05.413331Z",
     "start_time": "2024-12-15T13:16:05.406961Z"
    }
   },
   "source": [
    "def train(model, train_dataset, batch_size=512, epochs=10, learning_rate=0.01):\n",
    "  \"\"\"\n",
    "  This function implements the training logic\n",
    "  Inputs:\n",
    "  - model: the model ot be trained\n",
    "  - train_dataset: the training set of type NERDataset\n",
    "  - batch_size: integer represents the number of examples per step\n",
    "  - epochs: integer represents the total number of epochs (full training pass)\n",
    "  - learning_rate: the learning rate to be used by the optimizer\n",
    "  \"\"\"\n",
    "\n",
    "  ############################## TODO: replace the Nones in the following code ##################################\n",
    "  \n",
    "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "  # (2) make the criterion cross entropy loss\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # (3) create the optimizer (Adam)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "  # print(torch.cuda.get_device_name(device))\n",
    "  if use_cuda:\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.cuda(device)\n",
    "    pass\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "  \n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "  \n",
    "      # (4) move the train input to the device\n",
    "      train_input = train_input.to(device)\n",
    "  \n",
    "      # (5) move the train label to the device\n",
    "      train_label = train_label.to(device)\n",
    "  \n",
    "  \n",
    "      # (6) do the forward pass\n",
    "      output = model.forward(train_input)\n",
    "      \n",
    "      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "      batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n",
    "  \n",
    "      # (8) append the batch loss to the total_loss_train\n",
    "      total_loss_train += batch_loss\n",
    "      \n",
    "      # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "      acc = (torch.argmax(output, dim=-1) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "  \n",
    "      # (10) zero your gradients\n",
    "      optimizer.zero_grad()\n",
    "  \n",
    "      # (11) do the backward pass\n",
    "      batch_loss.backward()\n",
    "  \n",
    "      # (12) update the weights with your optimizer\n",
    "      optimizer.step()\n",
    "      \n",
    "    # epoch loss\n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "  \n",
    "    # (13) calculate the accuracy\n",
    "    sample_count = len(train_dataset)\n",
    "    seq_length = train_dataset[0][0].shape[0]\n",
    "    epoch_acc = total_acc_train / (sample_count * seq_length)\n",
    "  \n",
    "  \n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n",
    "      \n",
    "  ##############################################################################################################"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3BI7_ANkLf7G",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:16:25.206824Z",
     "start_time": "2024-12-15T13:16:07.526119Z"
    }
   },
   "source": [
    "train_dataset = NERDataset(t_sentences, t_labels, vocab['<PAD>'], tags[\"NONE\"])\n",
    "val_dataset = NERDataset(dev_sentences, dev_labels, vocab['<PAD>'], tags[\"NONE\"])\n",
    "test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'], tags[\"NONE\"])"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:16:54.505040Z",
     "start_time": "2024-12-15T13:16:54.469872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NER(embedding_dim=70, hidden_size=800, n_classes=len(tags), vocab_size=len(vocab))\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER(\n",
      "  (embedding): Embedding(307, 70)\n",
      "  (lstm): LSTM(70, 800, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (linear): Linear(in_features=1600, out_features=17, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:51:35.036337Z",
     "start_time": "2024-12-15T13:51:31.935685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(model, train_dataset, epochs=2, batch_size=32)\n",
    "train(model, val_dataset, epochs=5, batch_size=32)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 71.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.02098259888589382         | Train Accuracy: 0.9890845070422535\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 73.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.015567203983664513         | Train Accuracy: 0.9913145539906103\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 73.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.015058251097798347         | Train Accuracy: 0.9923415492957747\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 74.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.010789778083562851         | Train Accuracy: 0.9922535211267606\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 72.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.009715814143419266         | Train Accuracy: 0.992781690140845\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWJNO6mUXPRI"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gz5mxUAJM1xS",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:50:59.822708Z",
     "start_time": "2024-12-15T13:50:59.817205Z"
    }
   },
   "source": [
    "def evaluate(model, test_dataset, batch_size=64):\n",
    "  \"\"\"\n",
    "  This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "  Inputs:\n",
    "  - model: a NER model\n",
    "  - test_dataset: dataset of type NERDataset\n",
    "  \"\"\"\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size,shuffle=False)\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for test_input, test_label in tqdm(test_dataloader):\n",
    "      test_input = test_input.to(device)\n",
    "      test_label = test_label.to(device)\n",
    "      output = model.forward(test_input)\n",
    "\n",
    "      # Check if entire sequence matches by comparing all positions\n",
    "      sequence_matches = (torch.argmax(output, dim=-1) == test_label).all(dim=-1)\n",
    "      acc = sequence_matches.sum().item()\n",
    "      total_acc_test += acc\n",
    "    \n",
    "    total_acc_test /= len(test_dataset)\n",
    "  print(f'\\nTest Accuracy: {total_acc_test}')"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FD8JNcHWmMY",
    "outputId": "b4916766-dd57-4716-db7f-90c6d46655fa",
    "ExecuteTime": {
     "end_time": "2024-12-15T13:51:44.100619Z",
     "start_time": "2024-12-15T13:51:43.953597Z"
    }
   },
   "source": "evaluate(model, test_dataset)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 163.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.8732394366197183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:53:25.800493Z",
     "start_time": "2024-12-15T13:53:25.793176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inv_order_tags = {}\n",
    "for tag, value in tags.items():\n",
    "  inv_order_tags[value] = tag\n",
    "  \n",
    "def test_sample(sample):\n",
    "  s = [vocab[token] if token in vocab\n",
    "                 else vocab['<UNK>']\n",
    "                 for token in sample.split(' ') if token != '']\n",
    "  x_tensor = torch.tensor(s)\n",
    "  device = torch.device(\"cuda:0\")\n",
    "  output = model.forward(x_tensor.to(device))\n",
    "  output = torch.argmax(output, dim=-1).to(\"cpu\")\n",
    "  print([inv_order_tags[x.item()] for x in output])\n",
    "\n",
    "\n",
    "test_sample(\"two chicago style pizzas with pepperoni and tomato sauce\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NUMBER_S', 'STYLE_S', 'STYLE', 'NONE', 'NONE', 'TOPPING_S', 'NONE', 'TOPPING_S', 'TOPPING']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:53:42.507709Z",
     "start_time": "2024-12-15T13:53:42.471202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model_state(model, path):\n",
    "    \"\"\"\n",
    "    Saves only the model's parameters\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model_state(model, path):\n",
    "    \"\"\"\n",
    "    Loads the model's parameters into a pre-defined architecture\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "save_model_state(model, \"models/pizza_order_h800_x87.3.pth\")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
