{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Dataset given path to JSON file\n",
    "# input: JSON file   -> output: list of size 4 (sentence, EXR, TOP, TOP_DECOUPLED) * number of strings\n",
    "# we will use this to read the Training/ evaluation / test datasets. \n",
    "def read_dataset(path: str):\n",
    "    data = pd.read_json(path, lines = True)\n",
    "    columns = data.columns.tolist()\n",
    "    parsed_json = [None]*len(columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        parsed_json[i] = data[columns[i]] # IDK will it be easier to us to work with pandas or numpy\n",
    "    return parsed_json # we store data in list of PD.Series for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SERIES of String sentences\n",
    "def pre_text_normalization(sentences : pd.Series, flag = -1):\n",
    "# Words to Lower\n",
    "    # because i use this function in parsing the labels so i don't want it to affect\n",
    "    if flag == -1:\n",
    "        sentences = sentences.str.lower()\n",
    "# SIZES\n",
    "# after asking the TA, stating one format isn't a good idea so i won't standaradize the format\n",
    "# so things like Party - size , party size to standaradize this i will just remove the '-'\n",
    "    sentences = sentences.str.replace(r\"-\",\" \",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\s{2}\",r\" \",regex=True)\n",
    "# sometimes they refer to pizza as pie : WE ONLY SELL PIZZA\n",
    "    sentences = sentences.str.replace(\"pie\", \"pizza\")\n",
    "\n",
    "# Gluten - free we can leave it like that for now (may standardize it to gluten_free in future)\n",
    "\n",
    "# Quantities\n",
    "    '''\n",
    "    Now i want to take into consider quantities that means less of topping\n",
    "    something like \n",
    "    not much, not many, a little bit of, just a tiny bit of, just a bit, only a little, just a little, a bit, a little\n",
    "    we will leave those quantities like that for now and see in the future if we will change them\n",
    "    for quantities that mean much:\n",
    "    a lot of, lots of\n",
    "    '''\n",
    "\n",
    "# Quantity like \"a\" pizza should be converted to \"one\" , only one, just one -> one\n",
    "    sentences = sentences.str.replace(r\"\\ban?(?!\\s+(bit|tiny|lot|little))\\b\",\"one\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\b(?:only|just)\\sone\\b\",\"one\",regex=True)\n",
    "    \n",
    "# there are alot of Quantitative items 3 pies, Three pies ..\n",
    "# normalize digits to words \n",
    "    sentences = sentences.str.replace(r\"\\b([0-9]+)\\b\",lambda match: num2words(int(match.group(1))),regex=True)\n",
    "    \n",
    "# Negation\n",
    "    '''\n",
    "    There is multiple ways of negation, what i found while searching:\n",
    "    Without, hold the, With no(t), no, avoid, hate\n",
    "    i want complex words like (hold the , without) to be converted int no\n",
    "    we won't change those for now because i want to try learn the context of negation\n",
    "    '''\n",
    "# TOPPINGS \n",
    "# (I think BBQ topping needs to be paired with things, it's always written as bbq_chicken, bbq_sauce, bbq_pulled_pork...)\n",
    "# i think this is oversimplification and i will let the sequence model decide this\n",
    "# To be decided later\n",
    "\n",
    "# DRINKS\n",
    "# sometimes people say pepsi, sometimes pepsis so i don't want plurals -> let's stem\n",
    "    sentences = sentences.str.replace(r\"\\b(\\w\\w+)e?s\\b\",r\"\\1\",regex=True)\n",
    "# sometimes san pellegrino is said pellegrino only\n",
    "    sentences = sentences.str.replace(r\"\\bsan\\s(pellegrino)\\b\",r\"\\1\",regex=True)\n",
    "# sometimes wrote zeros as zeroe\n",
    "    sentences = sentences.str.replace(r\"\\b(zero)e\\b\",r\"\\1\",regex=True)\n",
    "# sometimes people write iced instead of ice\n",
    "    sentences = sentences.str.replace(r\"\\b(ice)d\\b\",r\"\\1\",regex=True)\n",
    "# DOCTOR PEPPER convert dr to doctor , peper to pepper\n",
    "    sentences = sentences.str.replace(r\"\\bdr\\b\",r\"doctor\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\bpeper\\b\",r\"pepper\",regex=True)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer \n",
    "def snow_ball_stemmer(vocab):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    if isinstance(vocab,set):\n",
    "        vocab = set([stemmer.stem(word) for word in vocab])\n",
    "        return vocab\n",
    "    else:\n",
    "        vocab = vocab.apply(lambda words: [stemmer.stem(word) for word in words])\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SET of vocab and , SERIES of list of tokens\n",
    "def tokenization(sentences: pd.Series):\n",
    "    # merge the whole series int one sentence to make the vocab extracting faster\n",
    "    all_words = ' '.join(sentences)\n",
    "    # used penn treebank tokenizer\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    all_words = tokenizer.tokenize(all_words)\n",
    "    \n",
    "    # keep the unique \n",
    "    vocab = set(all_words)\n",
    "    \n",
    "    sentences = sentences.apply(tokenizer.tokenize)\n",
    "\n",
    "    sentences.fillna(\"\",inplace=True)\n",
    "    # convert tokenized_sentences into padded lists so that they have same dimension\n",
    "    max_length = sentences.map(len).max()\n",
    "    padded_tokenized_sentences = sentences.apply(lambda x: x + [np.nan] * (max_length - len(x)))\n",
    "    padded_tokenized_sentences = pd.DataFrame(padded_tokenized_sentences.tolist())\n",
    "    padded_tokenized_sentences.fillna(0,inplace = True)\n",
    "   \n",
    "   # negation check regex : \\b(?<=not?)(.*?)(?=(\\.|,|$|and))\\b (for the future maybe ?)\n",
    "    return vocab, padded_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pizza_drinks(parsed_tree: pd.Series): # the tree is a SERIES of format that is like this (ORDER (DRINK,))....\n",
    "\n",
    "# i extract PIZZAORDER node if exist, and DRINKORDER node if exist\n",
    "    \n",
    "    pizza_orders, drink_orders = None, None\n",
    "    \n",
    "    \n",
    "    order_pattern = r\"(?<=\\(ORDER)(.*)(?=\\))\"   \n",
    "    # (ORDER i want to eat (PIZZAORDER)) This regex will extract i want to eat\n",
    "    extracted_words_before_parsing = r\"(?:(?:\\(ORDER\\s+)|(?:\\)))([^()]+)(?=[\\s(]+)\"\n",
    "    \n",
    "    none_words = parsed_tree.str.extractall(extracted_words_before_parsing).iloc[:,0].str.strip()\n",
    "    \n",
    "    none_words = none_words.dropna().reset_index(drop=True)\n",
    "\n",
    "    # remove the (ORDER and it's closing parenthesis at the end to ease next step by using order_pattern\n",
    "    extracted_orders = parsed_tree.str.extractall(order_pattern).iloc[:,0].str.strip()\n",
    "\n",
    "    # this regex leads to 2 capture groups : anything after PIZZAORDER and before ) and anything after DRINKORDER and before )\n",
    "    # # match non capturing group (PIZZA ORDER someshit) if exist, and match non capturing group (DRINKORDER someshit) if exist\n",
    "    # why non capturing? because i don't want the PD.extract to put it in the resulted Dataframe\n",
    "    # in each group : search for (PIZZAORDER then space 0 more -it should be 1- then match \"(\" then\n",
    "    # anything that isn't \")\" one or more -words- then space 0 or more then ) then space 0 or more IF EXIST same for DRINK  \n",
    "    pizza_drink_order_patterns = r\" (?:\\(PIZZAORDER\\s*((?:\\(?[^\\)]+\\)?\\s*)*)\\)\\s*)?(?:\\(DRINKORDER\\s*((?:\\(?[^\\)]+\\)?\\s*)*)\\)\\s*)? \"\n",
    "\n",
    "    # This is a dataframe of two series where 0 : pizza orders , 1: drink order\n",
    "    extracted_PIZZA_DRINK = parsed_tree.str.extractall(pizza_drink_order_patterns)\n",
    "    \n",
    "    pizza_orders = extracted_PIZZA_DRINK[0]\n",
    "    \n",
    "    drink_orders = extracted_PIZZA_DRINK[1]\n",
    "    # remove the sentences where the user didn't order drinks\n",
    "    drink_orders = drink_orders.dropna().reset_index(drop=True)\n",
    "    # remove the sentences where the user didn't order pizzas\n",
    "    pizza_orders = pizza_orders.dropna().reset_index(drop=True)\n",
    "    # (PIZZAORDER pizza (TOPPING)) Now i want pizza to be captured\n",
    "    # so i will encapsulate it with (NONE \\w+), i will process this before training\n",
    "    # so that words like pizza goes to pizza class, can goes to drink_container class\n",
    "    pizza_orders = pizza_orders.replace(r\"(?<=\\))(.*?)(?=\\()\",r\"(NONE \\1)\",regex=True)\n",
    "\n",
    "    # because the previous pattern may match spaces between brackets as words\n",
    "    # (TOPPing habd)\\s(Topping habd2) it will make \\s encapsulated and become (NONE \\s)\n",
    "    # so remove it here\n",
    "\n",
    "    pizza_orders = pizza_orders.replace(r\"\\(NONE\\s*\\)\", \"\",regex=True)\n",
    "    # same for drink_orders\n",
    "    drink_orders = drink_orders.replace(r\"(?<=\\))(.*?)(?=\\()\",r\"(NONE \\1)\",regex=True)\n",
    "\n",
    "    drink_orders = drink_orders.replace(r\"\\(NONE\\s*\\)\", \"\",regex=True)\n",
    "    # clean the ram \n",
    "    del extracted_orders\n",
    "    del extracted_PIZZA_DRINK\n",
    "    # return series of pizzaorders (TOPPING)(STYLE....), series of drinkorders of same format\n",
    "    # series of none_words i, 'd, want, .... \n",
    "    return pizza_orders, drink_orders, none_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a pd.Series of format (TOPPING)(STYLE)...\n",
    "# returns the words under every label\n",
    "def extract_nodes(pizza_orders:pd.Series,drink_orders:pd.Series):\n",
    "    drink_nodes, pizza_nodes = [] ,[]\n",
    "    if np.any(pizza_orders) :\n",
    "        pizza_node_attributes = [\"NUMBER\",\"SIZE\",\"NONE\",\"TOPPING\",\"QUANTITY\",\"STYLE\"]\n",
    "        for attribute in pizza_node_attributes:\n",
    "            node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "            pizza_nodes.append(pizza_orders.str.extract(node_pattern))\n",
    "            \n",
    "    if np.any(drink_orders) :\n",
    "        drink_node_attributes = [\"NUMBER\",\"SIZE\",\"NONE\", \"DRINKTYPE\",\"CONTAINERTYPE\",\"VOLUME\"]\n",
    "        for attribute in drink_node_attributes:\n",
    "            node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "            drink_nodes.append(drink_orders.str.extract(node_pattern))\n",
    "    return pizza_nodes, drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_nodes(pizza_nodes: list[pd.Series], drink_nodes: list[pd.Series]):\n",
    "    # i want to refine the extracted nodes since the one parsed from previous step has\n",
    "    # alot of nans so i will drop those, normalize the text and drop the duplicates\n",
    "    # after this step i can start labling the text\n",
    "    new_pizza_nodes, new_drink_nodes = [], []\n",
    "    for i in range(0,6):\n",
    "        # \n",
    "        node = pizza_nodes[i].dropna().reset_index(drop=True)\n",
    "        if i < 3: # There is SIZE, Number, None_Words for drink also so process on both\n",
    "            node = pd.concat([node,drink_nodes[i].dropna().reset_index(drop=True)],axis =0, ignore_index=True)\n",
    "        # convert the node from Dataframe of one series to one series \n",
    "        # so that we can use the series.str methods\n",
    "        node = node.iloc[:,0]\n",
    "        # if there was duplicates drop it to make normalization faster\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        \n",
    "        node = pre_text_normalization(node)\n",
    "        # after normalization duplicates will appear so delete them\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        \n",
    "        node = node.reset_index(drop=True)\n",
    "        # ensure there is no spaces\n",
    "        node = node.str.strip()\n",
    "\n",
    "        new_pizza_nodes.append(node)\n",
    "    # we already processed on size, number, none words so process on the remaining\n",
    "    for i in range(3,6):\n",
    "        # same for Drinks\n",
    "        node = drink_nodes[i].dropna().reset_index(drop=True)\n",
    "        \n",
    "        node = node.iloc[:,0]\n",
    "        \n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        \n",
    "        node = pre_text_normalization(node)\n",
    "        \n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        \n",
    "        node = node.reset_index(drop=True)\n",
    "        \n",
    "        node = node.str.strip()\n",
    "        \n",
    "        new_drink_nodes.append(node)\n",
    "\n",
    "    return new_pizza_nodes, new_drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(vocab):\n",
    "    unlabeled_vocab = vocab.to_numpy().reshape(-1,1)\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    \n",
    "    encoder = encoder.fit(unlabeled_vocab)\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_vocab(vocab: pd.DataFrame):\n",
    "    if isinstance(vocab, type(None)):\n",
    "        vocab = pd.read_csv(\"vocab.csv\")\n",
    "   \n",
    "    # add unknown for future when testing eval set\n",
    "   \n",
    "    vocab.loc[-1] = \"unk\"\n",
    "\n",
    "    vocab = vocab.reset_index(drop=True)\n",
    "\n",
    "    vocab[\"1\"] = \"none\"\n",
    "    # because pizza, negation aren't put within () in preprocessing\n",
    "    \n",
    "    # i put them by myself to remove them from the None set\n",
    "    labels = [None, None, None, None, None, None, None, None, None, pd.Series(\"pizza\"), pd.Series([\"hold\",\"without\",\"no\",\"avoid\",\"hate\",\"ani\"])]\n",
    "\n",
    "    csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\",\"volume\"]\n",
    "\n",
    "    for i, csv in zip(range(0,len(labels)), csv_file_names):\n",
    "        labels[i] = pd.read_csv(f\"./labels/{csv}.csv\").iloc[:,0]\n",
    "        labels[i] = labels[i].str.strip()\n",
    "        \n",
    "    for i in range(0,11):\n",
    "        if i != 2:\n",
    "            labels[2] = labels[2][~labels[2].isin(labels[i])]\n",
    "            labels[2].dropna(inplace=True)\n",
    "            labels[2] = labels[2].reset_index(drop=True)\n",
    "    for i in range(1,11):\n",
    "        labels[i] = labels[i][~labels[i].isin(labels[0])]\n",
    "\n",
    "    csv_file_names.extend([\"pizza\",\"neg\"])\n",
    "    for i in range(0,11):\n",
    "        vocab.loc[vocab[\"0\"].isin(labels[i]),\"1\"] = csv_file_names[i]\n",
    " \n",
    "    # returns vocab against labels\n",
    "    vocab_encoder = one_hot_encoding(vocab[vocab.columns[0]])\n",
    "\n",
    "# this will be as used as our target outputs \n",
    "    label_encoder = one_hot_encoding(pd.Series(csv_file_names))\n",
    "\n",
    "    encoded_tokens = vocab_encoder.transform(vocab[\"0\"].to_numpy().reshape(-1,1))\n",
    "    encoded_labels = label_encoder.transform(vocab[\"1\"].to_numpy().reshape(-1,1))\n",
    "    vocab.rename(columns={\"0\": \"tokens\",\"1\": \"labels\"},inplace=True)\n",
    "\n",
    "    vocab[\"encoded_tokens\"] = pd.Series([x.toarray().argmax(axis=1)[0] for x in encoded_tokens])\n",
    "    vocab[\"encoded_labels_array\"] = pd.Series([x.toarray()[0] for x in encoded_labels])\n",
    "    vocab[\"encoded_labels\"] = pd.Series([x.toarray().argmax(axis=1)[0] for x in encoded_labels])\n",
    "    \n",
    "    # write for future purposes instead of going through this loop again\n",
    "    vocab.to_csv(\"labeled entities.csv\",index=False)\n",
    "\n",
    "    return vocab, vocab_encoder, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conversions():\n",
    "    def __init__(self,vocab):\n",
    "        self.token_to_id = dict(zip(vocab[\"tokens\"], vocab[\"encoded_tokens\"]))\n",
    "        self.token_to_label = dict(zip(vocab[\"tokens\"], vocab[\"encoded_labels_array\"]))\n",
    "        \n",
    "        self.id_to_token = dict(zip(vocab[\"encoded_tokens\"],vocab[\"tokens\"]))\n",
    "        self.id_to_label = dict(zip(vocab[\"encoded_labels\"],vocab[\"labels\"]))\n",
    "        \n",
    "    def word2id(self,word):\n",
    "        return self.token_to_id.get(word, None)\n",
    "    def word2labels(self,word):\n",
    "        return self.token_to_label.get(word, np.zeros(11))\n",
    "    def id2token(self,number):\n",
    "        print(number)\n",
    "        return self.id_to_token.get(int(number),\"unk\")\n",
    "    def id2label(self,number):\n",
    "        return self.id_to_label.get(number, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the DataSet class from pytorch to facilitate \n",
    "# batch divisions and data preparation\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, input_indices, labels):\n",
    "        self.input_indices = input_indices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_indices[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pass_size_as_arg(size):\n",
    "    def integer_to_one_hot(index):\n",
    "        # Create a zero vector of length num_classes\n",
    "        one_hot = torch.zeros(size).type(torch.float32)\n",
    "        # Set the position corresponding to the index to 1\n",
    "        one_hot[int(index)] = 1\n",
    "        return one_hot\n",
    "    return integer_to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_size):\n",
    "        super(RNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size,embedding_dim=100).to(device)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # batch_first = True means that batch is the first dimension\n",
    "        # shape : batch_first, seq, input_size\n",
    "        self.lstm = nn.LSTM(input_size=100,hidden_size=hidden_size, batch_first=True).to(device)\n",
    "        # linear layer : from hidden RNN to Output\n",
    "        self.fc = nn.Linear(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is batch, seq cuz it's integer indices\n",
    "        mask_condition = input >0\n",
    "        input = input *mask_condition\n",
    "        \n",
    "        embed = self.embedding(input)\n",
    "        # for LSTM we need initial tensor state + initial hidden state\n",
    "        # where initial tensor state is called (cell)\n",
    "        # 1 : num of layers , batch size , hidden_size\n",
    "        c_0 = torch.zeros(1,input.size(0),self.hidden_size).to(device)\n",
    "        h_0 = torch.zeros(1,input.size(0),self.hidden_size).to(device)\n",
    "        # output of self.rnn : out feature, hidden_state(n)\n",
    "        out, _ = self.lstm(embed,(h_0,c_0))\n",
    "        # size of output = batch, seq_length, hidden_size\n",
    "        out = self.fc(out)\n",
    "        probabilities = torch.softmax(out, dim=2)\n",
    "        \n",
    "        return probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
