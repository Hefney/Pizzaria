{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"vocab.csv\")\n",
    "\n",
    "vocab.loc[-1] = \"unk\"\n",
    "vocab = vocab.reset_index(drop=True)\n",
    "vocab[\"1\"] = \"none\"\n",
    "\n",
    "labels = [None, None, None, None, None, None, None, None, None, pd.Series(\"pizza\"), pd.Series([\"hold\",\"without\",\"no\",\"avoid\",\"hate\",\"ani\"])]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\",\"volume\"]\n",
    "\n",
    "for i, csv in zip(range(0,len(labels)), csv_file_names):\n",
    "    labels[i] = pd.read_csv(f\"./labels/{csv}.csv\").iloc[:,0]\n",
    "    labels[i] = labels[i].str.strip()\n",
    "    \n",
    "for i in range(0,11):\n",
    "    if i != 2:\n",
    "        labels[2] = labels[2][~labels[2].isin(labels[i])]\n",
    "        labels[2].dropna(inplace=True)\n",
    "        labels[2] = labels[2].reset_index(drop=True)\n",
    "for i in range(1,11):\n",
    "    labels[i] = labels[i][~labels[i].isin(labels[0])]\n",
    "\n",
    "csv_file_names.extend([\"pizza\",\"neg\"])\n",
    "for i in range(0,11):\n",
    "    vocab.loc[vocab[\"0\"].isin(labels[i]),\"1\"] = csv_file_names[i]\n",
    "vocab.to_csv(\"labeled entities.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(vocab):\n",
    "    unlabeled_vocab = vocab.to_numpy().reshape(-1,1)\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    \n",
    "    encoder = encoder.fit(unlabeled_vocab)\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be used later to train the NN embedding\n",
    "vocab_encoder = one_hot_encoding(vocab[vocab.columns[0]])\n",
    "\n",
    "# this will be as used as our target outputs \n",
    "label_encoder = one_hot_encoding(pd.Series(csv_file_names))\n",
    "\n",
    "encoded_labels = label_encoder.transform(vocab[\"1\"].to_numpy().reshape(-1,1))\n",
    "encoded_tokens = vocab_encoder.transform(vocab[\"0\"].to_numpy().reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del labels\n",
    "del csv_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.rename(columns={\"0\": \"tokens\",\"1\": \"labels\"},inplace=True)\n",
    "\n",
    "vocab[\"encoded_tokens\"] = pd.Series([x.toarray()[0] for x in encoded_tokens])\n",
    "vocab[\"encoded_labels\"] = pd.Series([x.toarray()[0] for x in encoded_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'d\"]]\n",
      "[['none']]\n"
     ]
    }
   ],
   "source": [
    "# made sure that the encoding is correct\n",
    "print(vocab_encoder.inverse_transform([vocab[vocab[\"tokens\"] == \"'d\"].loc[vocab[vocab[\"tokens\"] == \"'d\"].index[0],\"encoded_tokens\"]]))\n",
    "print(label_encoder.inverse_transform([vocab[vocab[\"tokens\"] == \"'d\"].loc[vocab[vocab[\"tokens\"] == \"'d\"].index[0],\"encoded_labels\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2number(array):\n",
    "    if isinstance(array, np.ndarray):\n",
    "        return array.argmax(axis=0)\n",
    "    else:\n",
    "        return 1000 # it should be an impossible number so that i can use it as pad\n",
    "    # but because i will save this on disk, saving it as a number will make it easy for me \n",
    "    # and my vocab is 230 so i will never reach 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = dict(zip(vocab[\"tokens\"], vocab[\"encoded_tokens\"]))\n",
    "\n",
    "token_to_id[None] = np.zeros(223)\n",
    "token_to_label = dict(zip(vocab[\"tokens\"], vocab[\"encoded_labels\"]))\n",
    "token_to_label[None] = np.zeros(11)\n",
    "\n",
    "codes = vocab[\"encoded_tokens\"].map(array2number)\n",
    "\n",
    "id_to_token = dict(zip(codes,vocab[\"tokens\"]))\n",
    "\n",
    "codes = vocab[\"encoded_labels\"].map(array2number)\n",
    "\n",
    "id_to_label = dict(zip(codes,vocab[\"labels\"]))\n",
    "\n",
    "def word2id(word):\n",
    "    return token_to_id.get(word, None)\n",
    "def word2labels(word):\n",
    "      return token_to_label.get(word, None)\n",
    "def id2token(number):\n",
    "     return id_to_token.get(number,\"unk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza\n",
      "cheeseburg\n",
      "unk\n"
     ]
    }
   ],
   "source": [
    "# check conversions works correctly\n",
    "print(id2token(array2number(word2id(\"pizza\"))))\n",
    "print(id2token(array2number(word2id(\"cheeseburg\"))))\n",
    "#unknown word\n",
    "print(id2token(array2number(word2id(\"pizzas\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the DataSet class from pytorch to facilitate \n",
    "# batch divisions and data preparation\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, input_indices, labels):\n",
    "        self.input_indices = input_indices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_indices[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_length, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_length,embedding_dim, device=torch.device(\"cuda\"))\n",
    "        self.hidden_to_output = nn.Linear(embedding_dim, 11, bias= False, device= torch.device(\"cuda\"))\n",
    "  \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # this function has a problem Not correct for now\n",
    "    def forward(self, input):\n",
    "        hidden = self.embed(input)\n",
    "\n",
    "        hidden_mean = hidden.mean(dim = 1 )\n",
    "\n",
    "        output = self.hidden_to_output(hidden_mean)\n",
    "        \n",
    "        output = nn.functional.log_softmax(output, dim = -1)\n",
    "       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(embedding_dim,vocab, encoded_tokens,encoded_labels):\n",
    "# The nn.embedding take Indices by which it returns vectors\n",
    "\n",
    "# returns the integer representation of the one hot vectors -> needed by nn.embedding\n",
    "    input_indices  = encoded_tokens.toarray()\n",
    "    output_indices = encoded_labels.toarray()\n",
    "\n",
    "    tokens_tensor = torch.from_numpy(input_indices).type(torch.long).to(torch.device(\"cuda\"))\n",
    "    label_tensor  = torch.from_numpy(output_indices).type(torch.float32).to(torch.device(\"cuda\"))\n",
    "\n",
    "# to be able to train the nn.Embedding on our data\n",
    "# we inherit from nn.Module and construct the custom word embedding \n",
    "# what we do : \n",
    "# ONE hot encodes input dimension = Vocab -> Neural network dimension = Embedding ->output to hidden ->  dimension = Labels \n",
    "# use softmax to get what label\n",
    "\n",
    "\n",
    "# number of iterations (epoch is a standard way of saying iteration)\n",
    "# each epoch has N(4) batches so the loop should run 400 times\n",
    "    epochs =100\n",
    "\n",
    "    dataset = SimpleDataset(tokens_tensor,label_tensor)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=4 )\n",
    "\n",
    "    model = CustomWordEmbedding(vocab.shape[0],100)\n",
    "\n",
    "    # stochastic gradient descent\n",
    "    optimizer = torch.optim.SGD(model.embed.parameters(), lr = 0.0001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for batch_id, (indices, labels) in enumerate(dataloader):\n",
    "            outputs = model(indices)\n",
    "            \n",
    "            loss = model.loss(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print loss for each batch\n",
    "            if (batch_id + 1) % 2 == 0:  # Print every 2 batches\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_id+1}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "\n",
    "        # Optionally, print loss for every epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed, Loss: {loss.item()}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's common to use embedding size = 100 \n",
    "# embedding neural network takes 2 sizes : Input (No. of vocab since i use one hot encoding )\n",
    "# Hidden layer size : embedding\n",
    "# Doesn't work correctly for now\n",
    "# embed = train_embedding(100,vocab,encoded_tokens,encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my objective is to convert sequence of words into tensors\n",
    "# first read the data \n",
    "\n",
    "tokenized_sentences = tokenized_sentences.reset_index(drop = True)\n",
    "tokenized_sentences = tokenized_sentences[\"train.SRC\"].str.replace(r\"([\\[\\],\\\"]|'(?!d\\b))\", \"\",regex=True)\n",
    "tokenized_sentences = tokenized_sentences.str.split(\" \",expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenized_sentences.map(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenized_sentences.map(word2labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_as_numpy = ids.to_numpy()\n",
    "labels_as_numpy = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['larg']]\n",
      "[['size']]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check i did encode correctly: \n",
    "series = vocab[vocab[\"tokens\"] == vocab_encoder.inverse_transform([ids_as_numpy[0][4]])[0][0]]\n",
    "\n",
    "index = series.index[0]\n",
    "\n",
    "series1, series2 = series.loc[index,\"encoded_labels\"] , series.loc[index,\"encoded_tokens\"] \n",
    "\n",
    "print(vocab_encoder.inverse_transform([series2]))\n",
    "print(label_encoder.inverse_transform([series1]))\n",
    "print(len(tokenized_sentences.loc[0]) == len(tokenized_sentences.loc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_as_numpy_fixed = np.array(ids_as_numpy.tolist(), dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ids\n",
    "del tokenized_sentences\n",
    "del ids_as_numpy\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.from_numpy(ids_as_numpy_fixed).type(torch.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tokens_tensor,\"tokens.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ids_as_numpy_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_numpy_fixed = np.array(labels_as_numpy.tolist(), dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "del labels_as_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tensor = torch.from_numpy(labels_as_numpy_fixed).type(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(label_tensor,\"labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "del label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
