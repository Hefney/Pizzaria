{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils.ipynb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 READ THE DATA\n",
    "read_dataset: takes path to JSON file that has sentences, _.EXR, _.TOP, _.TOP_DECOUPLED\n",
    "and returns them as pandas.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need them so free the data\n",
    "del parsed_tree\n",
    "del decoupled_structured_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# STEP 2 : preprocess data\n",
    "##### What we should take into consideration? \n",
    "1- Word Normalization  \n",
    "2- Word Tokenization  \n",
    "Why we won't use Sentence segmentation?  \n",
    "It's useless, orders are one sentence question no clear punctuation exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "normalized_sentence = pre_text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "vocab, tokenized_sentences = tokenization(normalized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint: Normalization and tokenization of sentences\n",
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentences.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Parse data and extract labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pizza_orders, drink_orders, none_words = extract_pizza_drinks(structured_sentence.copy())\n",
    "\n",
    "none_words = none_words.drop_duplicates()\n",
    "\n",
    "none_words = pre_text_normalization(none_words)\n",
    "\n",
    "none_words = none_words.reset_index(drop=True)\n",
    "\n",
    "nones, _ = tokenization(none_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_nodes, drink_nodes = extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = clean_extracted_nodes(pizza_nodes, drink_nodes)\n",
    "number, size, none, topping , quantity, style = pizza_nodes\n",
    "drink_type, container_type, volume = drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [number, size, none, topping, quantity, style, drink_type, container_type, volume]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\",\"volume\"]\n",
    "# merge nones that came from inside the PIZZAORDER, DRINKORDER and what u got from outside them\n",
    "none_vocab = nones\n",
    "for label, csv in zip(labels, csv_file_names):\n",
    "    vocab, _ = tokenization(label)\n",
    "    if csv == \"none\":\n",
    "        vocab.update(none_vocab)\n",
    "    vocab = pd.Series(list(vocab))\n",
    "    vocab.to_csv(f\"./labels/{csv}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del labels\n",
    "del csv_file_names\n",
    "del pizza_nodes\n",
    "del pizza_orders\n",
    "del drink_nodes\n",
    "del drink_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Encode The tokens and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_encoder, label_encoder = create_labeled_vocab(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pizza']\n",
      "['pizza']\n"
     ]
    }
   ],
   "source": [
    "# made sure that the encoding is correct\n",
    "print(vocab_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"pizza\"].loc[vocab[vocab[\"tokens\"] == \"pizza\"].index[0],\"encoded_tokens\"]])])\n",
    "print(label_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"pizza\"].loc[vocab[vocab[\"tokens\"] == \"pizza\"].index[0],\"encoded_labels\"]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdo_\\AppData\\Local\\Temp\\ipykernel_19216\\1794801712.py:1: DtypeWarning: Columns (14,15,16,17,18,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertor = conversions(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_ids = tokenized_sentences.map(convertor.word2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_ids = tokenized_sentences.map(convertor.word2labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here on screw Pandas we only work with numpy, and tensors\n",
    "tokens_ids_as_numpy = tokens_as_ids.to_numpy()\n",
    "tokens_labels_as_numpy = labels_as_ids.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenized_sentences\n",
    "del tokens_as_ids\n",
    "del labels_as_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_numpy_fixed = np.array(tokens_labels_as_numpy.tolist(), dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "number\n"
     ]
    }
   ],
   "source": [
    "# check i did encode correctly:\n",
    "encode_test = tokens_ids_as_numpy[0][~np.isnan(tokens_ids_as_numpy[0])] \n",
    "series = vocab[vocab[\"tokens\"] == vocab_encoder.categories_[0][int(tokens_ids_as_numpy[0][3])] ]\n",
    "\n",
    "index = series.index[0]\n",
    "\n",
    "series1, series2 = series.loc[index,\"encoded_labels\"] , series.loc[index,\"encoded_tokens\"] \n",
    "\n",
    "print(vocab_encoder.categories_[0][series2])\n",
    "print(label_encoder.categories_[0][series1])\n",
    "# print(len(tokenized_sentences.loc[0]) == len(tokenized_sentences.loc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the memory will be bad when we transform the numpy to tensor\n",
    "# we need to split them and save them on disk so we can load batches when we train\n",
    "tokens_batches = np.array_split(tokens_ids_as_numpy,10)\n",
    "labels_batches = np.array_split(labels_as_numpy_fixed,10)\n",
    "del tokens_ids_as_numpy\n",
    "del tokens_labels_as_numpy\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(tokens_batches):\n",
    "    tensor_batch = torch.from_numpy(batch).type(torch.float32)\n",
    "    torch.save(tensor_batch,f\"./tokens_tensors/tokens_batch_{i}.pt\")\n",
    "for i, batch in enumerate(labels_batches):\n",
    "    tensor_batch = torch.from_numpy(batch).type(torch.float32)\n",
    "    torch.save(tensor_batch,f\"./labels_tensors/labels_batch_{i}.pt\")\n",
    "del tensor_batch\n",
    "del tokens_batches\n",
    "del labels_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what to run my tensors on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# my one hot encoders DETERMINSTIC ON WHAT CRITERIA I WILL TRAIN ON\n",
    "input_size = vocab.shape[0]\n",
    "# this is a parameter \n",
    "hidden_size = 300\n",
    "# output : num of classes\n",
    "num_classes = 11\n",
    "# num of trials (epochs)\n",
    "epochs = 10\n",
    "# Batch size = ? \n",
    "batch_size = 10\n",
    "# learning_rate\n",
    "lr = 0.01\n",
    "\n",
    "# num_layers in RNN default is 1 (increasing layers improve result but worsen the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# What is the model i will use\n",
    "model = RNN(input_size,num_classes,hidden_size)\n",
    "# loss criteria here i use CEloss\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "# stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr)\n",
    "# they say decaying learning rate is better than fixed one so i will use learning rate scheduler\n",
    "lambdalr = lambda epoch: epoch / 10\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer,lambdalr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:, loss =3.162358522415161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for i in range(9):\n",
    "        tokens = torch.load(f\"./tokens_tensors/tokens_batch_{i}.pt\",weights_only=True).type(torch.int)\n",
    "        labels = torch.load(f\"./labels_tensors/labels_batch_{i}.pt\",weights_only=True).type(torch.int)\n",
    "\n",
    "        dataset = SimpleDataset(tokens,labels)\n",
    "        data = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "        \n",
    "        del tokens\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for input_tensors, label_tensors in data:\n",
    "            \n",
    "            input_tensors = input_tensors.to(device)\n",
    "            \n",
    "            label_tensors = label_tensors.to(device)\n",
    "           \n",
    "            out_tensor = model(input_tensors)\n",
    "            \n",
    "            mask_condition = label_tensors > 0\n",
    "\n",
    "            label_tensors = (label_tensors * mask_condition).type(torch.float32)\n",
    "\n",
    "            loss = loss_criterion(out_tensor,label_tensors)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        # Here should be the evaluation after every epoch\n",
    "        # no grad so that pytorch doesn't insert it in his calculations\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            # this is a hold-k evaluation (where we hold k from training set and evaluate based on it )\n",
    "            # till i parse the evaluation\n",
    "            tokens = torch.load(f\"./tokens_tensors/tokens_batch_{9}.pt\",weights_only=True).type(torch.int)\n",
    "            labels = torch.load(f\"./labels_tensors/labels_batch_{9}.pt\",weights_only=True).type(torch.int)\n",
    "    \n",
    "            for inputs, labels in data:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=2)  # Shape: (batch_size, seq_length, features) dim = 2 : features\n",
    "\n",
    "            # Flatten predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(torch.argmax(labels,dim=2).cpu().numpy().flatten())\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        report = classification_report(all_labels, all_preds)\n",
    "        print(f\"epoch {epoch}'s Accuracy:\", accuracy)\n",
    "        print(\"Classification Report:\\n\", report)\n",
    "    scheduler.step()\n",
    "    print(f\"epoch {epoch}:, loss ={loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
