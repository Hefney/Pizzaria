{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk import pos_tag\n",
    "import importlib\n",
    "import re\n",
    "from word2number import w2n\n",
    "import pickle\n",
    "importlib.reload(utils)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 READ THE DATA\n",
    "read_dataset: takes path to JSON file that has sentences, _.EXR, _.TOP, _.TOP_DECOUPLED\n",
    "and returns them as pandas.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = utils.read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't need them so free the data\n",
    "del parsed_tree\n",
    "del decoupled_structured_sentence\n",
    "del structured_sentence\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Parse data and extract labels\n",
    "In this step we build our Multiword expressions, extract the labels of every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this if u didn't read the pizza_train.json above\n",
    "structured_sentence = pd.read_csv(\"TOP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this if u read the TOP.csv\n",
    "structured_sentence = structured_sentence.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pizza_orders, drink_orders, none_words = utils.extract_pizza_drinks(structured_sentence.copy())\n",
    "\n",
    "none_words = none_words.drop_duplicates()\n",
    "\n",
    "none_words = utils.pre_text_normalization(none_words)\n",
    "\n",
    "none_words = none_words.reset_index(drop=True)\n",
    "\n",
    "nones, _ = utils.tokenization(none_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none words have a problem\n",
    "# words like pizza, Negations are in this set -> extract them\n",
    "pizza_class = \"pizza\"\n",
    "negation_class = [\"hold\", \"avoid\", \"hate\", \"without\", \"no\",\"not\"]\n",
    "if pizza_class in nones:\n",
    "    nones.remove(pizza_class)\n",
    "for word in negation_class:\n",
    "    if word in nones:\n",
    "        nones.remove(word)\n",
    "# here we save this nones, pizza, negation_classes\n",
    "pd.Series(negation_class).to_csv(f\"./labels/negation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_nodes, drink_nodes = utils.extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = utils.clean_extracted_nodes(pizza_nodes, drink_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_number, pizza_size, topping , quantity, style = pizza_nodes\n",
    "drink_number, drink_size, drink_type, container_type, volume = drink_nodes\n",
    "number = pd.concat([pizza_number,drink_number])\n",
    "size = pd.concat([pizza_size,drink_size])\n",
    "number.drop_duplicates(inplace=True)\n",
    "size.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the nodes to contain only the meaningful words\n",
    "vocab, _ = utils.tokenization(number)\n",
    "number_vocab = set()\n",
    "for word in vocab:\n",
    "    try:\n",
    "        _ = w2n.word_to_num(word)\n",
    "        number_vocab.add(word)\n",
    "    except ValueError:\n",
    "        nones.add(word)\n",
    "number = pd.Series(list(number_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, _ = utils.tokenization(volume)\n",
    "volume_vocab = set()\n",
    "for word in vocab:\n",
    "    try:\n",
    "        _ = w2n.word_to_num(word)\n",
    "        number[-1] = word\n",
    "        number.reset_index(drop=True,inplace=True)\n",
    "        number.drop_duplicates(inplace=True)\n",
    "    except ValueError:\n",
    "        volume_vocab.add(word)\n",
    "volume = pd.Series(list(volume_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to remove extra from size\n",
    "size_vocab, _ = utils.tokenization(size)\n",
    "topping_vocab, _ = utils.tokenization(topping)\n",
    "quantity_vocab, _ = utils.tokenization(quantity)\n",
    "style_vocab, _ = utils.tokenization(style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for style in style_vocab.copy():\n",
    "    if style in topping_vocab:\n",
    "        style_vocab.remove(style)\n",
    "    if style in nones:\n",
    "        style_vocab.remove(style)\n",
    "for size in size_vocab.copy():\n",
    "    if size in quantity_vocab:\n",
    "        size_vocab.remove(size)\n",
    "nones.update({\"only\", \"just\",\"a\",\"of\"})\n",
    "quantity_vocab.remove(\"only\")\n",
    "quantity_vocab.remove(\"just\")\n",
    "quantity_vocab.remove(\"a\")\n",
    "quantity_vocab.remove(\"of\")\n",
    "quantity_vocab.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = pd.Series(list(size_vocab))\n",
    "quantity = pd.Series(list(quantity_vocab))\n",
    "style = pd.Series(list(style_vocab))\n",
    "topping = pd.Series(list(topping_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for drink types i will use multiword tokenizer \n",
    "# from where will he invent new drink ?\n",
    "drink_type_vocab, tokens = utils.tokenization(drink_type)\n",
    "mwe =[]\n",
    "vocab = set()\n",
    "for col in tokens.columns:\n",
    "        tokens.loc[tokens[col] == 0,col] = \"\"\n",
    "tokens = tokens.to_numpy().tolist()\n",
    "for i,token_list in enumerate(tokens):\n",
    "        while \"\" in token_list:\n",
    "                token_list.remove(\"\")\n",
    "        if len(token_list) == 1:\n",
    "                vocab.add(token_list[0])\n",
    "        else:\n",
    "                mwe.append(tuple(token_list))\n",
    "                string = \"_\".join(token_list)\n",
    "                string = re.sub(r\"_+$\",\"\",string)\n",
    "                vocab.add(string)\n",
    "drink_type_vocab = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MWETokenizer(mwe)\n",
    "with open(\"DRINK_MWE_TOKENS.pkl\", 'wb') as file:\n",
    "    pickle.dump(tokenizer,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_vocab, _ = utils.tokenization(container_type)\n",
    "container_vocab.remove(\"a\")\n",
    "container_vocab.remove(\"in\")\n",
    "nones.add(\"in\")\n",
    "container_type = pd.Series(list(container_vocab))\n",
    "drink_type = pd.Series(list(drink_type_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_vocab = pd.Series(list(volume_vocab))\n",
    "volume_vocab.to_csv(f\"./labels/volume.csv\", index=False)\n",
    "nones = pd.Series(list(nones))\n",
    "labels = [number, size, nones, topping, quantity, style, drink_type, container_type, volume]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\"]\n",
    "for vocab, csv in zip(labels,csv_file_names):\n",
    "    vocab.to_csv(f\"./labels/{csv}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [number, size, nones, topping, quantity, style, drink_type, container_type, volume]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\"]\n",
    "# merge nones that came from inside the PIZZAORDER, DRINKORDER and what u got from outside them\n",
    "none_vocab = nones\n",
    "mwe =[]\n",
    "for label, csv in zip(labels, csv_file_names):\n",
    "    if csv != \"none\":\n",
    "        _, tokens = utils.tokenization(label)\n",
    "        tokens.drop_duplicates(inplace=True)\n",
    "        vocab = set()\n",
    "        for col in tokens.columns:\n",
    "            tokens.loc[tokens[col] == 0,col] = \"\"\n",
    "        tokens = tokens.to_numpy().tolist()\n",
    "        for i,token_list in enumerate(tokens):\n",
    "            while \"\" in token_list:\n",
    "                token_list.remove(\"\")\n",
    "            if len(token_list) != 1:\n",
    "                mwe.append(tuple(token_list))\n",
    "                string = \" \".join(token_list)\n",
    "                string = re.sub(r\"\\s$\",\"\",string)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the state of the MWETOKENIZER to be used when processing the sentences\n",
    "tokenizer = MWETokenizer(mwe)\n",
    "with open(\"MWE_TOKENS.pkl\", 'wb') as file:\n",
    "    pickle.dump(tokenizer,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del labels\n",
    "del csv_file_names\n",
    "del pizza_nodes\n",
    "del pizza_orders\n",
    "del drink_nodes\n",
    "del drink_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# STEP 3 : preprocess data\n",
    "##### What we should take into consideration? \n",
    "1- Word Normalization  \n",
    "2- Word Tokenization  \n",
    "Why we won't use Sentence segmentation?  \n",
    "It's useless, orders are one sentence question no clear punctuation exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "normalized_sentence = utils.pre_text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1293560,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_sentence.drop_duplicates(inplace=True)\n",
    "normalized_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "vocab, _ = utils.tokenization(normalized_sentence,tokenizesentences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4214,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling 20 sentence for every word in vocab, I hope this makes acceptable result\n",
    "# the sampling will take 2 - 4 minutes (225 word * sample cost)\n",
    "sampled_normalized_sentences = pd.Series()\n",
    "for word in vocab:\n",
    "    x = word\n",
    "    sampled = normalized_sentence[normalized_sentence.str.contains(x)].to_numpy()\n",
    "    np.random.shuffle(sampled)\n",
    "    random_sample = sampled[:20]\n",
    "    series = pd.Series(random_sample)\n",
    "    sampled_normalized_sentences = pd.concat([sampled_normalized_sentences,series],axis=0)\n",
    "\n",
    "sampled_normalized_sentences.drop_duplicates(inplace=True)\n",
    "sampled_normalized_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, tokenized_sentences = utils.tokenization(sampled_normalized_sentences, tokenizesentences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint: Normalization and tokenization of sentences\n",
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentences.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'), ('want', 'VBP'), ('one', 'CD'), ('large', 'JJ'), ('pizza', 'NN')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to run pos_tag you need to download the nltk_data first -> nltk.download()\n",
    "sentence = tokenized_sentences.iloc[1]\n",
    "sentence.replace([\"0\",0],\"PAD\",inplace=True)\n",
    "pos_tag([\"i\",\"want\",\"one\",\"large\",\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Encode The tokens and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_encoder, label_encoder = utils.create_labeled_vocab(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bbq']\n",
      "['topping']\n"
     ]
    }
   ],
   "source": [
    "# made sure that the encoding is correct\n",
    "print(vocab_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"bbq\"].loc[vocab[vocab[\"tokens\"] == \"bbq\"].index[0],\"encoded_tokens\"]])])\n",
    "print(label_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"bbq\"].loc[vocab[vocab[\"tokens\"] == \"bbq\"].index[0],\"encoded_labels\"]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")\n",
    "tokenized_sentences.replace([0,\"0\"],\"PAD\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertor = utils.conversions(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_ids = tokenized_sentences.map(convertor.word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_ids = tokenized_sentences.map(convertor.word2labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_ids.replace(1,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# لخبطة المودل هغير كام كلمة واحطهم ب UNK\n",
    "unknown_id = convertor.word2id(\"UNK\")\n",
    "words_for_perplexion = [\"ounce\",\"peppperoni\",\"fourteen\",\"napolitana\",\"broccoli\"]\n",
    "for i, word in enumerate(words_for_perplexion):\n",
    "    words_for_perplexion[i] = convertor.word2id(word)\n",
    "    tokens_as_ids.replace(words_for_perplexion[i],unknown_id,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here on screw Pandas we only work with numpy, and tensors\n",
    "tokens_ids_as_numpy = tokens_as_ids.to_numpy()\n",
    "tokens_labels_as_numpy = labels_as_ids.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1997"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenized_sentences\n",
    "del tokens_as_ids\n",
    "del labels_as_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "# check i did encode correctly:\n",
    "encode_test = tokens_ids_as_numpy[0][~np.isnan(tokens_ids_as_numpy[0])] \n",
    "series = vocab[vocab[\"tokens\"] == vocab_encoder.categories_[0][int(tokens_ids_as_numpy[0][3])] ]\n",
    "\n",
    "index = series.index[0]\n",
    "\n",
    "series1, series2 = series.loc[index,\"encoded_labels\"] , series.loc[index,\"encoded_tokens\"] \n",
    "\n",
    "print(vocab_encoder.categories_[0][series2])\n",
    "print(label_encoder.categories_[0][series1])\n",
    "# print(len(tokenized_sentences.loc[0]) == len(tokenized_sentences.loc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the memory will be bad when we transform the numpy to tensor\n",
    "# we need to split them and save them on disk so we can load batches when we train\n",
    "tokens_tensor = torch.from_numpy(tokens_ids_as_numpy).type(torch.float32)\n",
    "torch.save(tokens_tensor,f\"./tokens_tensors.pt\")\n",
    "labels_tensors = torch.from_numpy(tokens_labels_as_numpy).type(torch.float32)\n",
    "torch.save(labels_tensors,f\"./labels_tensors.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n"
     ]
    }
   ],
   "source": [
    "print(vocab.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what to run my tensors on\n",
    "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
    "# my one hot encoders DETERMINSTIC ON WHAT CRITERIA I WILL TRAIN ON\n",
    "input_size = vocab.shape[0]\n",
    "# this is a parameter \n",
    "hidden_size = 512\n",
    "# output : num of classes\n",
    "num_classes = 12\n",
    "# num of trials (epochs)\n",
    "epochs = 20\n",
    "# Batch size = ? \n",
    "batch_size = 8\n",
    "# learning_rate\n",
    "lr = 0.5\n",
    "\n",
    "# num_layers in RNN default is 1 (increasing layers improve result but worsen the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# What is the model i will use\n",
    "model = utils.RNN(input_size,num_classes,hidden_size)\n",
    "# loss criteria here i use CEloss\n",
    "loss_criterion = utils.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "# stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr, momentum=0.9)\n",
    "# they say decaying learning rate is better than fixed one so i will use learning rate scheduler\n",
    "lambdalr = lambda epoch: epoch / 10\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer,lambdalr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens = torch.load(f\"./tokens_tensors.pt\",weights_only=True).type(torch.int64)\n",
    "labels = torch.load(f\"./labels_tensors.pt\",weights_only=True).type(torch.int64)\n",
    "\n",
    "dataset = utils.SimpleDataset(tokens,labels)\n",
    "        \n",
    "        \n",
    "data = DataLoader(dataset,batch_size=batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0's Accuracy: 0.09057413476086765\n",
      "epoch 0:, loss =5307.789555549622\n",
      "epoch 1's Accuracy: 0.12119454225949372\n",
      "epoch 1:, loss =163.5378072372987\n",
      "epoch 2's Accuracy: 0.12000668308602523\n",
      "epoch 2:, loss =3.20824131160407\n",
      "epoch 3's Accuracy: 0.1176818511809047\n",
      "epoch 3:, loss =1.2558851849371422\n",
      "epoch 4's Accuracy: 0.11607333075508695\n",
      "epoch 4:, loss =1.6797350928343349\n",
      "epoch 5's Accuracy: 0.1033498544647764\n",
      "epoch 5:, loss =2.146426518692806\n",
      "epoch 6's Accuracy: 0.11548677970241608\n",
      "epoch 6:, loss =0.9004268102767128\n",
      "epoch 7's Accuracy: 0.1113156180667223\n",
      "epoch 7:, loss =3.05394951676044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(),max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss=0\n",
    "    model.train()\n",
    "\n",
    "    for input_tensors, label_tensors in data:\n",
    "            \n",
    "        input_tensors = input_tensors.to(device)\n",
    "            \n",
    "        label_tensors = label_tensors.to(device)\n",
    "           \n",
    "        out_tensor = model(input_tensors)\n",
    "        \n",
    "        loss = loss_criterion(out_tensor.view(-1,out_tensor.shape[-1]),label_tensors.view(-1))\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Here should be the evaluation after every epoch\n",
    "        # no grad so that pytorch doesn't insert it in his calculations\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # this is a hold-k evaluation (where we hold k from training set and evaluate based on it )\n",
    "        # till i parse the evaluation\n",
    "        tokens = torch.load(f\"./tokens_tensors/tokens_batch_{0}.pt\",weights_only=True).type(torch.int)\n",
    "        labels = torch.load(f\"./labels_tensors/labels_batch_{0}.pt\",weights_only=True).type(torch.int)\n",
    "        eval_dataset = utils.SimpleDataset(tokens,labels)\n",
    "        eval_data = DataLoader(eval_dataset,batch_size=1024)\n",
    "        for inputs, labels in eval_data:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=-1)  # Shape: (batch_size, seq_length, features) dim = 2 == -1 : features\n",
    "\n",
    "            # Flatten predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"epoch {epoch}'s Accuracy:\", accuracy)\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"epoch {epoch}:, loss ={total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word after tokenization:\n",
      " ['one', 'pepsi', 'in', 'a', 'bottle', 'and', 'also', 'one', 'unk', 'sauce', 'pizza']\n",
      "number drink_type none none container_type none none number volume style pizza "
     ]
    }
   ],
   "source": [
    "yarab = \"one pepsi in a bottle and also one UNK sauce pizza\"\n",
    "yarab = pd.Series(yarab)\n",
    "yarab = utils.pre_text_normalization(yarab)\n",
    "_, yarab = utils.tokenization(yarab)\n",
    "print( \"word after tokenization:\\n\", list(yarab.iloc[0]))\n",
    "yarab = yarab.map(convertor.word2id)\n",
    "yarab = yarab.to_numpy()\n",
    "plz = torch.from_numpy(yarab).to(device)\n",
    "out_plz = model(plz)\n",
    "out_plz = out_plz.argmax(dim =2)\n",
    "for plz2 in out_plz[0]:\n",
    "    print(convertor.id2label(plz2.item()),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
