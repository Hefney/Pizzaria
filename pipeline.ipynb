{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils.ipynb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 READ THE DATA\n",
    "read_dataset: takes path to JSON file that has sentences, _.EXR, _.TOP, _.TOP_DECOUPLED\n",
    "and returns them as pandas.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need them so free the data\n",
    "del parsed_tree\n",
    "del decoupled_structured_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# STEP 2 : preprocess data\n",
    "##### What we should take into consideration? \n",
    "1- Word Normalization  \n",
    "2- Word Tokenization  \n",
    "Why we won't use Sentence segmentation?  \n",
    "It's useless, orders are one sentence question no clear punctuation exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "normalized_sentence = pre_text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "vocab, tokenized_sentences = tokenization(normalized_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint: Normalization and tokenization of sentences\n",
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentences.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Parse data and extract labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pizza_orders, drink_orders, none_words = extract_pizza_drinks(structured_sentence.copy())\n",
    "\n",
    "none_words = none_words.drop_duplicates()\n",
    "\n",
    "none_words = pre_text_normalization(none_words)\n",
    "\n",
    "none_words = none_words.reset_index(drop=True)\n",
    "\n",
    "nones, _ = tokenization(none_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_nodes, drink_nodes = extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = clean_extracted_nodes(pizza_nodes, drink_nodes)\n",
    "number, size, none, topping , quantity, style = pizza_nodes\n",
    "drink_type, container_type, volume = drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [number, size, none, topping, quantity, style, drink_type, container_type, volume]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\",\"volume\"]\n",
    "# merge nones that came from inside the PIZZAORDER, DRINKORDER and what u got from outside them\n",
    "none_vocab = nones\n",
    "for label, csv in zip(labels, csv_file_names):\n",
    "    vocab, _ = tokenization(label)\n",
    "    if csv == \"none\":\n",
    "        vocab.update(none_vocab)\n",
    "    vocab = pd.Series(list(vocab))\n",
    "    vocab.to_csv(f\"./labels/{csv}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del labels\n",
    "del csv_file_names\n",
    "del pizza_nodes\n",
    "del pizza_orders\n",
    "del drink_nodes\n",
    "del drink_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Encode The tokens and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_encoder, label_encoder = create_labeled_vocab(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pizza']\n",
      "['pizza']\n"
     ]
    }
   ],
   "source": [
    "# made sure that the encoding is correct\n",
    "print(vocab_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"pizza\"].loc[vocab[vocab[\"tokens\"] == \"pizza\"].index[0],\"encoded_tokens\"]])])\n",
    "print(label_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"pizza\"].loc[vocab[vocab[\"tokens\"] == \"pizza\"].index[0],\"encoded_labels\"]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdo_\\AppData\\Local\\Temp\\ipykernel_19216\\1794801712.py:1: DtypeWarning: Columns (14,15,16,17,18,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertor = conversions(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_ids = tokenized_sentences.map(convertor.word2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_ids = tokenized_sentences.map(convertor.word2labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here on screw Pandas we only work with numpy, and tensors\n",
    "tokens_ids_as_numpy = tokens_as_ids.to_numpy()\n",
    "tokens_labels_as_numpy = labels_as_ids.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenized_sentences\n",
    "del tokens_as_ids\n",
    "del labels_as_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_numpy_fixed = np.array(tokens_labels_as_numpy.tolist(), dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "number\n"
     ]
    }
   ],
   "source": [
    "# check i did encode correctly:\n",
    "encode_test = tokens_ids_as_numpy[0][~np.isnan(tokens_ids_as_numpy[0])] \n",
    "series = vocab[vocab[\"tokens\"] == vocab_encoder.categories_[0][int(tokens_ids_as_numpy[0][3])] ]\n",
    "\n",
    "index = series.index[0]\n",
    "\n",
    "series1, series2 = series.loc[index,\"encoded_labels\"] , series.loc[index,\"encoded_tokens\"] \n",
    "\n",
    "print(vocab_encoder.categories_[0][series2])\n",
    "print(label_encoder.categories_[0][series1])\n",
    "# print(len(tokenized_sentences.loc[0]) == len(tokenized_sentences.loc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the memory will be bad when we transform the numpy to tensor\n",
    "# we need to split them and save them on disk so we can load batches when we train\n",
    "tokens_batches = np.array_split(tokens_ids_as_numpy,10)\n",
    "labels_batches = np.array_split(labels_as_numpy_fixed,10)\n",
    "del tokens_ids_as_numpy\n",
    "del tokens_labels_as_numpy\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(tokens_batches):\n",
    "    tensor_batch = torch.from_numpy(batch).type(torch.float32)\n",
    "    torch.save(tensor_batch,f\"./tokens_tensors/tokens_batch_{i}.pt\")\n",
    "for i, batch in enumerate(labels_batches):\n",
    "    tensor_batch = torch.from_numpy(batch).type(torch.float32)\n",
    "    torch.save(tensor_batch,f\"./labels_tensors/labels_batch_{i}.pt\")\n",
    "del tensor_batch\n",
    "del tokens_batches\n",
    "del labels_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what to run my tensors on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# my one hot encoders DETERMINSTIC ON WHAT CRITERIA I WILL TRAIN ON\n",
    "input_size = vocab.shape[0]\n",
    "# this is a parameter \n",
    "hidden_size = 300\n",
    "# output : num of classes\n",
    "num_classes = 11\n",
    "# num of trials (epochs)\n",
    "epochs = 10\n",
    "# Batch size = ? \n",
    "batch_size = 10\n",
    "# learning_rate\n",
    "lr = 0.01\n",
    "\n",
    "# num_layers in RNN default is 1 (increasing layers improve result but worsen the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# What is the model i will use\n",
    "model = RNN(input_size,num_classes,hidden_size)\n",
    "# loss criteria here i use CEloss\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "# stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr)\n",
    "# they say decaying learning rate is better than fixed one so i will use learning rate scheduler\n",
    "lambdalr = lambda epoch: epoch / 10\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer,lambdalr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m label_tensors \u001b[38;5;241m=\u001b[39m (label_tensors \u001b[38;5;241m*\u001b[39m mask_condition)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_criterion(out_tensor,label_tensors)\n\u001b[1;32m---> 29\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:947\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_zero_grad_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_groups\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\profiler.py:733\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    for i in range(1):\n",
    "        tokens = torch.load(f\"./tokens_tensors/tokens_batch_{i}.pt\",weights_only=True).type(torch.int)\n",
    "        labels = torch.load(f\"./labels_tensors/labels_batch_{i}.pt\",weights_only=True).type(torch.int)\n",
    "\n",
    "        dataset = SimpleDataset(tokens,labels)\n",
    "        data = DataLoader(dataset,batch_size=batch_size)\n",
    "        \n",
    "        del tokens\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for input_tensors, label_tensors in data:\n",
    "            \n",
    "            input_tensors = input_tensors.to(device)\n",
    "            \n",
    "            label_tensors = label_tensors.to(device)\n",
    "           \n",
    "            out_tensor = model(input_tensors)\n",
    "            \n",
    "            mask_condition = label_tensors > 0\n",
    "\n",
    "            label_tensors = (label_tensors * mask_condition).type(torch.float32)\n",
    "\n",
    "            loss = loss_criterion(out_tensor,label_tensors)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    print(f\"epoch {epoch}:, loss ={loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
