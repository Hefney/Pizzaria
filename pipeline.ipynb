{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import importlib\n",
    "import re\n",
    "from word2number import w2n\n",
    "import pickle\n",
    "importlib.reload(utils)\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 READ THE DATA\n",
    "read_dataset: takes path to JSON file that has sentences, _.EXR, _.TOP, _.TOP_DECOUPLED\n",
    "and returns them as pandas.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = utils.read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need them so free the data\n",
    "del parsed_tree\n",
    "del decoupled_structured_sentence\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Parse data and extract labels\n",
    "In this step we build our Multiword expressions, extract the labels of every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this if u didn't read the pizza_train.json above\n",
    "structured_sentence = pd.read_csv(\"TOP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this if u read the TOP.csv\n",
    "structured_sentence = structured_sentence.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pizza_orders, drink_orders, none_words = utils.extract_pizza_drinks(structured_sentence.copy())\n",
    "\n",
    "none_words = none_words.drop_duplicates()\n",
    "\n",
    "none_words = utils.pre_text_normalization(none_words)\n",
    "\n",
    "none_words = none_words.reset_index(drop=True)\n",
    "\n",
    "nones, _ = utils.tokenization(none_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_nodes, drink_nodes = utils.extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = utils.clean_extracted_nodes(pizza_nodes, drink_nodes)\n",
    "pizza_number, pizza_size, pizza_none, topping , quantity, style = pizza_nodes\n",
    "drink_number, drink_size, drink_none, drink_type, container_type, volume = drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = pd.concat([pizza_number,drink_number])\n",
    "size = pd.concat([pizza_size,drink_size])\n",
    "none = pd.concat([pizza_none,drink_none])\n",
    "number.drop_duplicates(inplace=True)\n",
    "size.drop_duplicates(inplace=True)\n",
    "none.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will treat the volume differently than other nodes to make sure i take into consideration the measuring units only\n",
    "vocab, _ = utils.tokenization(volume)\n",
    "volume_vocab = set()\n",
    "for word in vocab:\n",
    "    try:\n",
    "        _ = w2n.word_to_num(word)\n",
    "        number[-1] = word\n",
    "        number.reset_index(drop=True,inplace=True)\n",
    "        number.drop_duplicates(inplace=True)\n",
    "    except ValueError:\n",
    "        volume_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_vocab = pd.Series(list(volume_vocab))\n",
    "volume_vocab.to_csv(f\"./labels/volume.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [number, size, none, topping, quantity, style, drink_type, container_type, volume]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\"]\n",
    "# merge nones that came from inside the PIZZAORDER, DRINKORDER and what u got from outside them\n",
    "none_vocab = nones\n",
    "mwe =[]\n",
    "for label, csv in zip(labels, csv_file_names):\n",
    "    if csv != \"none\":\n",
    "        _, tokens = utils.tokenization(label)\n",
    "        tokens.drop_duplicates(inplace=True)\n",
    "        vocab = set()\n",
    "        for col in tokens.columns:\n",
    "            tokens.loc[tokens[col] == 0,col] = \"\"\n",
    "        tokens = tokens.to_numpy().tolist()\n",
    "        for i,token_list in enumerate(tokens):\n",
    "            while \"\" in token_list:\n",
    "                token_list.remove(\"\")\n",
    "            if len(token_list) == 1:\n",
    "                vocab.add(token_list[0])\n",
    "            else:\n",
    "                mwe.append(tuple(token_list))\n",
    "                string = \"_\".join(token_list)\n",
    "                string = re.sub(\"_+$\",\"\",string)\n",
    "                vocab.add(string)\n",
    "    else:\n",
    "        vocab, _ = utils.tokenization(label)\n",
    "        vocab.update(none_vocab)\n",
    "\n",
    "    vocab = pd.Series(list(vocab))\n",
    "    vocab.to_csv(f\"./labels/{csv}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the state of the MWETOKENIZER to be used when processing the sentences\n",
    "tokenizer = MWETokenizer(mwe)\n",
    "with open(\"MWE_TOKENS.pkl\", 'wb') as file:\n",
    "    pickle.dump(tokenizer,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del labels\n",
    "del csv_file_names\n",
    "del pizza_nodes\n",
    "del pizza_orders\n",
    "del drink_nodes\n",
    "del drink_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# STEP 3 : preprocess data\n",
    "##### What we should take into consideration? \n",
    "1- Word Normalization  \n",
    "2- Word Tokenization  \n",
    "Why we won't use Sentence segmentation?  \n",
    "It's useless, orders are one sentence question no clear punctuation exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "normalized_sentence = utils.pre_text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "vocab, tokenized_sentences = utils.tokenization(normalized_sentence,tokenizesentences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint: Normalization and tokenization of sentences\n",
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentences.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Encode The tokens and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_encoder, label_encoder = utils.create_labeled_vocab(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made sure that the encoding is correct\n",
    "print(vocab_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"bbq_pulled_pork\"].loc[vocab[vocab[\"tokens\"] == \"bbq_pulled_pork\"].index[0],\"encoded_tokens\"]])])\n",
    "print(label_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"bbq_pulled_pork\"].loc[vocab[vocab[\"tokens\"] == \"bbq_pulled_pork\"].index[0],\"encoded_labels\"]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertor = utils.conversions(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_ids = tokenized_sentences.map(convertor.word2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_ids = tokenized_sentences.map(convertor.word2labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here on screw Pandas we only work with numpy, and tensors\n",
    "tokens_ids_as_numpy = tokens_as_ids.to_numpy()\n",
    "tokens_labels_as_numpy = labels_as_ids.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenized_sentences\n",
    "del tokens_as_ids\n",
    "del labels_as_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check i did encode correctly:\n",
    "encode_test = tokens_ids_as_numpy[0][~np.isnan(tokens_ids_as_numpy[0])] \n",
    "series = vocab[vocab[\"tokens\"] == vocab_encoder.categories_[0][int(tokens_ids_as_numpy[0][3])] ]\n",
    "\n",
    "index = series.index[0]\n",
    "\n",
    "series1, series2 = series.loc[index,\"encoded_labels\"] , series.loc[index,\"encoded_tokens\"] \n",
    "\n",
    "print(vocab_encoder.categories_[0][series2])\n",
    "print(label_encoder.categories_[0][series1])\n",
    "# print(len(tokenized_sentences.loc[0]) == len(tokenized_sentences.loc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the memory will be bad when we transform the numpy to tensor\n",
    "# we need to split them and save them on disk so we can load batches when we train\n",
    "tokens_batches = np.array_split(tokens_ids_as_numpy,10)\n",
    "labels_batches = np.array_split(tokens_labels_as_numpy,10)\n",
    "del tokens_ids_as_numpy\n",
    "del tokens_labels_as_numpy\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(tokens_batches):\n",
    "    tensor_batch = torch.from_numpy(batch).type(torch.float32)\n",
    "    torch.save(tensor_batch,f\"./tokens_tensors/tokens_batch_{i}.pt\")\n",
    "for i, batch in enumerate(labels_batches):\n",
    "    tensor_batch = torch.from_numpy(batch).type(torch.float32)\n",
    "    torch.save(tensor_batch,f\"./labels_tensors/labels_batch_{i}.pt\")\n",
    "del tensor_batch\n",
    "del tokens_batches\n",
    "del labels_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what to run my tensors on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# my one hot encoders DETERMINSTIC ON WHAT CRITERIA I WILL TRAIN ON\n",
    "input_size = vocab.shape[0]\n",
    "# this is a parameter \n",
    "hidden_size = 300\n",
    "# output : num of classes\n",
    "num_classes = 11\n",
    "# num of trials (epochs)\n",
    "epochs = 10\n",
    "# Batch size = ? \n",
    "batch_size = 10\n",
    "# learning_rate\n",
    "lr = 0.01\n",
    "\n",
    "# num_layers in RNN default is 1 (increasing layers improve result but worsen the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# What is the model i will use\n",
    "model = utils.RNN(input_size,num_classes,hidden_size)\n",
    "# loss criteria here i use CEloss\n",
    "loss_criterion = utils.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "# stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr)\n",
    "# they say decaying learning rate is better than fixed one so i will use learning rate scheduler\n",
    "lambdalr = lambda epoch: epoch / 10\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer,lambdalr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for i in range(9):\n",
    "        tokens = torch.load(f\"./tokens_tensors/tokens_batch_{i}.pt\",weights_only=True).type(torch.int64)\n",
    "        labels = torch.load(f\"./labels_tensors/labels_batch_{i}.pt\",weights_only=True).type(torch.int64)\n",
    "\n",
    "        dataset = utils.SimpleDataset(tokens,labels)\n",
    "        data = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "        \n",
    "        del tokens\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for input_tensors, label_tensors in data:\n",
    "            \n",
    "            input_tensors = input_tensors.to(device)\n",
    "            \n",
    "            label_tensors = label_tensors.to(device)\n",
    "           \n",
    "            out_tensor = model(input_tensors)\n",
    "            \n",
    "            loss = loss_criterion(out_tensor.view(-1,out_tensor.shape[-1]),label_tensors.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        # Here should be the evaluation after every epoch\n",
    "        # no grad so that pytorch doesn't insert it in his calculations\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            # this is a hold-k evaluation (where we hold k from training set and evaluate based on it )\n",
    "            # till i parse the evaluation\n",
    "            tokens = torch.load(f\"./tokens_tensors/tokens_batch_{9}.pt\",weights_only=True).type(torch.int)\n",
    "            labels = torch.load(f\"./labels_tensors/labels_batch_{9}.pt\",weights_only=True).type(torch.int)\n",
    "    \n",
    "            for inputs, labels in data:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=2)  # Shape: (batch_size, seq_length, features) dim = 2 : features\n",
    "\n",
    "            # Flatten predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(torch.argmax(labels,dim=2).cpu().numpy().flatten())\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"epoch {epoch}'s Accuracy:\", accuracy)\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"epoch {epoch}:, loss ={loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
