{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\abdo_\\\\Downloads\\\\programs\\\\NLP\\\\Pizzaria\\\\utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import importlib\n",
    "import re\n",
    "from word2number import w2n\n",
    "import pickle\n",
    "importlib.reload(utils)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 READ THE DATA\n",
    "read_dataset: takes path to JSON file that has sentences, _.EXR, _.TOP, _.TOP_DECOUPLED\n",
    "and returns them as pandas.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = utils.read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't need them so free the data\n",
    "del parsed_tree\n",
    "del decoupled_structured_sentence\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Parse data and extract labels\n",
    "In this step we build our Multiword expressions, extract the labels of every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this if u didn't read the pizza_train.json above\n",
    "structured_sentence = pd.read_csv(\"TOP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this if u read the TOP.csv\n",
    "structured_sentence = structured_sentence.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pizza_orders, drink_orders, none_words = utils.extract_pizza_drinks(structured_sentence.copy())\n",
    "\n",
    "none_words = none_words.drop_duplicates()\n",
    "\n",
    "none_words = utils.pre_text_normalization(none_words)\n",
    "\n",
    "none_words = none_words.reset_index(drop=True)\n",
    "\n",
    "nones, _ = utils.tokenization(none_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none words have a problem\n",
    "# words like pizza, Negations are in this set -> extract them\n",
    "pizza_class = \"pizza\"\n",
    "negation_class = [\"hold\", \"avoid\", \"hate\", \"without\", \"no\"]\n",
    "nones.remove(pizza_class)\n",
    "for word in negation_class:\n",
    "    nones.remove(word)\n",
    "# here we save this nones, pizza, negation_classes\n",
    "pd.Series(negation_class).to_csv(f\"./labels/negation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_nodes, drink_nodes = utils.extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = utils.clean_extracted_nodes(pizza_nodes, drink_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_number, pizza_size, topping , quantity, style = pizza_nodes\n",
    "drink_number, drink_size, drink_type, container_type, volume = drink_nodes\n",
    "number = pd.concat([pizza_number,drink_number])\n",
    "size = pd.concat([pizza_size,drink_size])\n",
    "number.drop_duplicates(inplace=True)\n",
    "size.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the nodes to contain only the meaningful words\n",
    "vocab, _ = utils.tokenization(number)\n",
    "number_vocab = set()\n",
    "for word in vocab:\n",
    "    try:\n",
    "        _ = w2n.word_to_num(word)\n",
    "        number_vocab.add(word)\n",
    "    except ValueError:\n",
    "        nones.add(word)\n",
    "number = pd.Series(list(number_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, _ = utils.tokenization(volume)\n",
    "volume_vocab = set()\n",
    "for word in vocab:\n",
    "    try:\n",
    "        _ = w2n.word_to_num(word)\n",
    "        number[-1] = word\n",
    "        number.reset_index(drop=True,inplace=True)\n",
    "        number.drop_duplicates(inplace=True)\n",
    "    except ValueError:\n",
    "        volume_vocab.add(word)\n",
    "volume = pd.Series(list(volume_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to remove extra from size\n",
    "size_vocab, _ = utils.tokenization(size)\n",
    "topping_vocab, _ = utils.tokenization(topping)\n",
    "quantity_vocab, _ = utils.tokenization(quantity)\n",
    "style_vocab, _ = utils.tokenization(style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for style in style_vocab.copy():\n",
    "    if style in topping_vocab:\n",
    "        style_vocab.remove(style)\n",
    "    if style in nones:\n",
    "        style_vocab.remove(style)\n",
    "for size in size_vocab.copy():\n",
    "    if size in quantity_vocab:\n",
    "        size_vocab.remove(size)\n",
    "nones.update({\"only\", \"just\"})\n",
    "quantity_vocab.remove(\"only\")\n",
    "quantity_vocab.remove(\"just\")\n",
    "quantity_vocab.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = pd.Series(list(size_vocab))\n",
    "quantity = pd.Series(list(quantity_vocab))\n",
    "style = pd.Series(list(style_vocab))\n",
    "topping = pd.Series(list(topping_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for drink types i will use multiword tokenizer \n",
    "# from where will he invent new drink ?\n",
    "drink_type_vocab, tokens = utils.tokenization(drink_type)\n",
    "mwe =[]\n",
    "vocab = set()\n",
    "for col in tokens.columns:\n",
    "        tokens.loc[tokens[col] == 0,col] = \"\"\n",
    "tokens = tokens.to_numpy().tolist()\n",
    "for i,token_list in enumerate(tokens):\n",
    "        while \"\" in token_list:\n",
    "                token_list.remove(\"\")\n",
    "        if len(token_list) == 1:\n",
    "                vocab.add(token_list[0])\n",
    "        else:\n",
    "                mwe.append(tuple(token_list))\n",
    "                string = \"_\".join(token_list)\n",
    "                string = re.sub(r\"_+$\",\"\",string)\n",
    "                vocab.add(string)\n",
    "drink_type_vocab = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MWETokenizer(mwe)\n",
    "with open(\"DRINK_MWE_TOKENS.pkl\", 'wb') as file:\n",
    "    pickle.dump(tokenizer,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_vocab, _ = utils.tokenization(container_type)\n",
    "container_vocab.remove(\"one\")\n",
    "container_vocab.remove(\"in\")\n",
    "nones.add(\"in\")\n",
    "container_type = pd.Series(list(container_vocab))\n",
    "drink_type = pd.Series(list(drink_type_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_vocab = pd.Series(list(volume_vocab))\n",
    "volume_vocab.to_csv(f\"./labels/volume.csv\", index=False)\n",
    "nones = pd.Series(list(nones))\n",
    "labels = [number, size, nones, topping, quantity, style, drink_type, container_type, volume]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\"]\n",
    "for vocab, csv in zip(labels,csv_file_names):\n",
    "    vocab.to_csv(f\"./labels/{csv}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [number, size, nones, topping, quantity, style, drink_type, container_type, volume]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\"]\n",
    "# merge nones that came from inside the PIZZAORDER, DRINKORDER and what u got from outside them\n",
    "none_vocab = nones\n",
    "mwe =[]\n",
    "for label, csv in zip(labels, csv_file_names):\n",
    "    if csv != \"none\":\n",
    "        _, tokens = utils.tokenization(label)\n",
    "        tokens.drop_duplicates(inplace=True)\n",
    "        vocab = set()\n",
    "        for col in tokens.columns:\n",
    "            tokens.loc[tokens[col] == 0,col] = \"\"\n",
    "        tokens = tokens.to_numpy().tolist()\n",
    "        for i,token_list in enumerate(tokens):\n",
    "            while \"\" in token_list:\n",
    "                token_list.remove(\"\")\n",
    "            if len(token_list) != 1:\n",
    "                mwe.append(tuple(token_list))\n",
    "                string = \" \".join(token_list)\n",
    "                string = re.sub(r\"\\s$\",\"\",string)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the state of the MWETOKENIZER to be used when processing the sentences\n",
    "tokenizer = MWETokenizer(mwe)\n",
    "with open(\"MWE_TOKENS.pkl\", 'wb') as file:\n",
    "    pickle.dump(tokenizer,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del labels\n",
    "del csv_file_names\n",
    "del pizza_nodes\n",
    "del pizza_orders\n",
    "del drink_nodes\n",
    "del drink_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# STEP 3 : preprocess data\n",
    "##### What we should take into consideration? \n",
    "1- Word Normalization  \n",
    "2- Word Tokenization  \n",
    "Why we won't use Sentence segmentation?  \n",
    "It's useless, orders are one sentence question no clear punctuation exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "normalized_sentence = utils.pre_text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "vocab, tokenized_sentences = utils.tokenization(normalized_sentence,tokenizesentences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint: Normalization and tokenization of sentences\n",
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentences.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab\n",
    "del tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Encode The tokens and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_encoder, label_encoder = utils.create_labeled_vocab(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bbq']\n",
      "['topping']\n"
     ]
    }
   ],
   "source": [
    "# made sure that the encoding is correct\n",
    "print(vocab_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"bbq\"].loc[vocab[vocab[\"tokens\"] == \"bbq\"].index[0],\"encoded_tokens\"]])])\n",
    "print(label_encoder.categories_[0][([vocab[vocab[\"tokens\"] == \"bbq\"].loc[vocab[vocab[\"tokens\"] == \"bbq\"].index[0],\"encoded_labels\"]])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdo_\\AppData\\Local\\Temp\\ipykernel_8124\\1459111782.py:1: DtypeWarning: Columns (13,14,15,16,17,18,19,20,21,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = pd.read_csv(\"tokenized_sentences.csv\")\n",
    "tokenized_sentences.replace([0,\"0\"],\"PAD\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertor = utils.conversions(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_ids = tokenized_sentences.map(convertor.word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_ids = tokenized_sentences.map(convertor.word2labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_ids.replace(1,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# لخبطة المودل هغير كام كلمة واحطهم ب UNK\n",
    "unknown_id = convertor.word2id(\"UNK\")\n",
    "words_for_perplexion = [\"ounce\",\"peppperoni\",\"fourteen\",\"napolitana\",\"broccoli\"]\n",
    "for i, word in enumerate(words_for_perplexion):\n",
    "    words_for_perplexion[i] = convertor.word2id(word)\n",
    "    tokens_as_ids.replace(words_for_perplexion[i],unknown_id,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here on screw Pandas we only work with numpy, and tensors\n",
    "tokens_ids_as_numpy = tokens_as_ids.to_numpy()\n",
    "tokens_labels_as_numpy = labels_as_ids.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenized_sentences\n",
    "del tokens_as_ids\n",
    "del labels_as_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "number\n"
     ]
    }
   ],
   "source": [
    "# check i did encode correctly:\n",
    "encode_test = tokens_ids_as_numpy[0][~np.isnan(tokens_ids_as_numpy[0])] \n",
    "series = vocab[vocab[\"tokens\"] == vocab_encoder.categories_[0][int(tokens_ids_as_numpy[0][3])] ]\n",
    "\n",
    "index = series.index[0]\n",
    "\n",
    "series1, series2 = series.loc[index,\"encoded_labels\"] , series.loc[index,\"encoded_tokens\"] \n",
    "\n",
    "print(vocab_encoder.categories_[0][series2])\n",
    "print(label_encoder.categories_[0][series1])\n",
    "# print(len(tokenized_sentences.loc[0]) == len(tokenized_sentences.loc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the memory will be bad when we transform the numpy to tensor\n",
    "# we need to split them and save them on disk so we can load batches when we train\n",
    "tokens_tensor = torch.from_numpy(tokens_ids_as_numpy).type(torch.float32)\n",
    "torch.save(tokens_tensor,f\"./tokens_tensors.pt\")\n",
    "labels_tensors = torch.from_numpy(tokens_labels_as_numpy).type(torch.float32)\n",
    "torch.save(labels_tensors,f\"./labels_tensors.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "print(vocab.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what to run my tensors on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# my one hot encoders DETERMINSTIC ON WHAT CRITERIA I WILL TRAIN ON\n",
    "input_size = vocab.shape[0]\n",
    "# this is a parameter \n",
    "hidden_size = 400\n",
    "# output : num of classes\n",
    "num_classes = 12\n",
    "# num of trials (epochs)\n",
    "epochs = 20\n",
    "# Batch size = ? \n",
    "batch_size = 32\n",
    "# learning_rate\n",
    "lr = 0.5\n",
    "\n",
    "# num_layers in RNN default is 1 (increasing layers improve result but worsen the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# What is the model i will use\n",
    "model = utils.RNN(input_size,num_classes,hidden_size)\n",
    "# loss criteria here i use CEloss\n",
    "loss_criterion = utils.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "# stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr)\n",
    "# they say decaying learning rate is better than fixed one so i will use learning rate scheduler\n",
    "lambdalr = lambda epoch: epoch / 10\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer,lambdalr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.load(f\"./tokens_tensors/tokens_batch_{4}.pt\",weights_only=True).type(torch.int64)\n",
    "labels = torch.load(f\"./tokens_tensors/tokens_batch_{4}.pt\",weights_only=True).type(torch.int64)\n",
    "\n",
    "dataset = utils.SimpleDataset(tokens,labels)\n",
    "subset_indices = torch.randperm(dataset.__len__())[:40000]\n",
    "subset_sampler = SubsetRandomSampler(subset_indices)\n",
    "data = DataLoader(dataset,batch_size=batch_size,sampler=subset_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0's Accuracy: 0.04274583333333333\n",
      "epoch 0:, loss =779.0084149837494\n",
      "epoch 1's Accuracy: 0.37904166666666667\n",
      "epoch 1:, loss =509.5594679117203\n",
      "epoch 2's Accuracy: 0.4665\n",
      "epoch 2:, loss =203.79303359985352\n",
      "epoch 3's Accuracy: 0.4982375\n",
      "epoch 3:, loss =76.54760914295912\n",
      "epoch 4's Accuracy: 0.5049958333333333\n",
      "epoch 4:, loss =29.761675644665956\n",
      "epoch 5's Accuracy: 0.5073208333333333\n",
      "epoch 5:, loss =12.42368852160871\n",
      "epoch 6's Accuracy: 0.5084125\n",
      "epoch 6:, loss =5.88608543202281\n",
      "epoch 7's Accuracy: 0.508675\n",
      "epoch 7:, loss =3.167883210349828\n",
      "epoch 8's Accuracy: 0.5088375\n",
      "epoch 8:, loss =1.8957276444416493\n",
      "epoch 9's Accuracy: 0.5088541666666667\n",
      "epoch 9:, loss =1.2433592724846676\n",
      "epoch 10's Accuracy: 0.5089583333333333\n",
      "epoch 10:, loss =0.8689509643008932\n",
      "epoch 11's Accuracy: 0.5089791666666666\n",
      "epoch 11:, loss =0.6338998822611757\n",
      "epoch 12's Accuracy: 0.5089833333333333\n",
      "epoch 12:, loss =0.48225429040030576\n",
      "epoch 13's Accuracy: 0.5089791666666666\n",
      "epoch 13:, loss =0.37513921910431236\n",
      "epoch 14's Accuracy: 0.5088833333333334\n",
      "epoch 14:, loss =0.3047934318310581\n",
      "epoch 15's Accuracy: 0.5090041666666667\n",
      "epoch 15:, loss =0.25544643178000115\n",
      "epoch 16's Accuracy: 0.5090083333333333\n",
      "epoch 16:, loss =0.20350781580782495\n",
      "epoch 17's Accuracy: 0.5090041666666667\n",
      "epoch 17:, loss =0.17374104718328454\n",
      "epoch 18's Accuracy: 0.5090125\n",
      "epoch 18:, loss =0.1472604875889374\n",
      "epoch 19's Accuracy: 0.5090125\n",
      "epoch 19:, loss =0.1298247856029775\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss=0\n",
    "    model.train()\n",
    "        \n",
    "    for input_tensors, label_tensors in data:\n",
    "            \n",
    "        input_tensors = input_tensors.to(device)\n",
    "            \n",
    "        label_tensors = label_tensors.to(device)\n",
    "           \n",
    "        out_tensor = model(input_tensors)\n",
    "            \n",
    "        loss = loss_criterion(out_tensor.view(-1,out_tensor.shape[-1]),label_tensors.view(-1))\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Here should be the evaluation after every epoch\n",
    "        # no grad so that pytorch doesn't insert it in his calculations\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # this is a hold-k evaluation (where we hold k from training set and evaluate based on it )\n",
    "        # till i parse the evaluation\n",
    "        tokens = torch.load(f\"./tokens_tensors/tokens_batch_{4}.pt\",weights_only=True).type(torch.int)\n",
    "        labels = torch.load(f\"./tokens_tensors/tokens_batch_{4}.pt\",weights_only=True).type(torch.int)\n",
    "        eval_dataset = utils.SimpleDataset(tokens,labels)\n",
    "        eval_data = DataLoader(eval_dataset,batch_size=1024)\n",
    "        for inputs, labels in data:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=2)  # Shape: (batch_size, seq_length, features) dim = 2 : features\n",
    "\n",
    "            # Flatten predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"epoch {epoch}'s Accuracy:\", accuracy)\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"epoch {epoch}:, loss ={total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n",
      "none\n",
      "number\n",
      "topping\n",
      "pizza\n",
      "none\n",
      "number\n",
      "number\n",
      "pizza\n"
     ]
    }
   ],
   "source": [
    "yarab = \"i want four cheese pizzas and i want\"\n",
    "yarab = pd.Series(yarab)\n",
    "yarab = utils.pre_text_normalization(yarab)\n",
    "_, yarab = utils.tokenization(yarab)\n",
    "yarab = yarab.map(convertor.word2id)\n",
    "yarab = yarab.to_numpy()\n",
    "plz = torch.from_numpy(yarab).to(device)\n",
    "out_plz = model(plz)\n",
    "out_plz = out_plz.argmax(dim =2)\n",
    "for plz2 in out_plz[0]:\n",
    "    print(convertor.id2label(plz2.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
