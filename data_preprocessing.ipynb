{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.945306Z",
     "start_time": "2024-11-29T16:16:11.593866Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "525e1e8c84335bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.951944Z",
     "start_time": "2024-11-29T16:16:15.947321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825ea706bbc1c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.960799Z",
     "start_time": "2024-11-29T16:16:15.953961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Dataset given path to JSON file\n",
    "# input: JSON file   -> output: list of size 4 (sentence, EXR, TOP, TOP_DECOUPLED) * number of strings\n",
    "# we will use this to read the Training/ evaluation / test datasets. \n",
    "def read_dataset(path: str):\n",
    "    data = pd.read_json(path, lines = True)\n",
    "    columns = data.columns.tolist()\n",
    "    parsed_json = [None]*len(columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        parsed_json[i] = data[columns[i]] # IDK will it be easier to us to work with pandas or numpy\n",
    "    return parsed_json # we store data in list of PD.Series for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "9974825debeb08f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.969130Z",
     "start_time": "2024-11-29T16:16:15.962816Z"
    }
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SERIES of String sentences\n",
    "def text_normalization(sentences):\n",
    "# convert words to lower\n",
    "    sentences = sentences.str.lower()\n",
    "# sizes are flexible some are party-sized, party size , lunch - sized ....\n",
    "# i will assume one format A-B : party-sized\n",
    "# now we build our regex where we search for \"(some shit) size\" and convert it to some shit-size \n",
    "    pattern_size =r'''(?xm) # multiline verbose flag to enable comments\n",
    "        \\b([a-z]*)[-\\s]*(size)(?:d)?\\b # i search for alphabets that is followed by one or more spaces or \"-\" then one or more spaces\n",
    "                                       # then size, not capturing d because i don't want it anymore (i don't want it in group actually)\n",
    "        '''\n",
    "    sentences = sentences.str.replace(pattern_size, r\"\\1_\\2\",regex=True)\n",
    "# sometimes they ask the pizza to be gluten-free, gluten free -> normalizing this to gluten-free\n",
    "# gluten-free is a complete style so i don't think that this will be removed\n",
    "    pattern_free = r'''(?xm) \n",
    "        \\b([a-z]*)[-\\s]*(free)\\b\n",
    "        '''\n",
    "    sentences = sentences.str.replace(pattern_free, r\"\\1_\\2\",regex=True)\n",
    "# sometimes they refer to pizza as pie : WE ONLY SELL PIZZA\n",
    "    sentences = sentences.str.replace(\"pie\", \"pizza\")\n",
    "# Quantities\n",
    "    '''\n",
    "    Now i want to take into consider quantities that means less of topping\n",
    "    something like not much not many .... we convert all those to a standard quantity : light\n",
    "    not much, not many, a little bit of, just a tiny bit of, just a bit, only a little, just a little, a bit, a little\n",
    "    '''\n",
    "    less_quantities = [r\"\\bnot\\sm(?:uch|any)\\b\",r\"\\b(only\\s|just\\s)?a\\slittle(\\sbit\\sof)?\\b\"\n",
    "                       ,r\"\\b(just\\s)?a\\s(tiny\\s)?bit(\\sof)?\\b\"]\n",
    "    for word in less_quantities:\n",
    "        sentences = sentences.str.replace(word,\"light\",regex=True)\n",
    "    '''\n",
    "    for quantities that mean much:\n",
    "    a lot of, lots of\n",
    "    '''\n",
    "    more_quantities =[r\"\\ba\\slot\\sof\\b\", r\"\\blots\\sof\\b\"] # any extra that doesn't have large after\n",
    "    for word in more_quantities:\n",
    "        sentences = sentences.str.replace(word,\"extra\",regex=True)\n",
    "# Quantity like \"a\" pizza should be converted to \"one\" , only one, just one -> one\n",
    "    sentences = sentences.str.replace(r\"\\ban?\\b\",\"one\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\b(?:only|just)\\sone\\b\",\"one\",regex=True)\n",
    "    \n",
    "# there are alot of Quantitative items 3 pies, Three pies ..\n",
    "# normalize words to digits only\n",
    "    word_to_num = {\n",
    "    r\"\\bone\\b\": \"1\", r\"\\btwo\\b\": \"2\", r\"\\bthree\\b\": \"3\", r\"\\bfour\\b\": \"4\", r\"\\bfive\\b\": \"5\",\n",
    "    r\"\\bsix\\b\": \"6\", r\"\\bseven\\b\": \"7\", r\"\\beight\\b\": \"8\", r\"\\bnine\\b\": \"9\", r\"\\bten\\b\": \"10\",\n",
    "    r\"\\beleven\\b\": \"11\", r\"\\btwelve\\b\": \"12\", r\"\\bthirteen\\b\": \"13\", r\"\\bfourteen\\b\": \"14\", r\"\\bfifteen\\b\": \"15\",\n",
    "    r\"\\bsixteen\\b\": \"16\"\n",
    "    }\n",
    "    sentences = sentences.replace(regex=[key for key in word_to_num.keys()],value=[value for value in word_to_num.values()])\n",
    "\n",
    "\n",
    "# Negation\n",
    "    '''\n",
    "    There is multiple ways of negation, what i found while searching:\n",
    "    Without, hold the, With no(t), no, avoid\n",
    "    i want complex words like (hold the , without) to be converted int no\n",
    "    i will use no to negate the whole toppings in the tokenizer\n",
    "    '''\n",
    "    negation_words = [r\"\\bwithout\\b\", r\"\\bhold\\sthe\\b\", r\"\\bavoid\\b\"]\n",
    "    for word in negation_words:\n",
    "        sentences = sentences.str.replace(word, \"no\" ,regex=True)\n",
    "# DRINKS\n",
    "# sometimes people say pepsi, sometimes pepsis so i don't want plurals -> let's stem\n",
    "    sentences = sentences.str.replace(r\"\\b(\\w\\w+)e?s\\b\",r\"\\1\",regex=True)\n",
    "# sometimes san pellegrino is said pellegrino only\n",
    "    sentences = sentences.str.replace(r\"\\bsan\\s(pellegrino)\\b\",r\"\\1\",regex=True)\n",
    "# sometimes wrote zeros as zeroe\n",
    "    sentences = sentences.str.replace(r\"\\b(zero)e\\b\",r\"\\1\",regex=True)\n",
    "# sometimes people write iced instead of ice\n",
    "    sentences = sentences.str.replace(r\"\\b(ice)d\\b\",r\"\\1\",regex=True)\n",
    "# DOCTOR PEPPER convert dr to doctor , peper to pepper\n",
    "    sentences = sentences.str.replace(r\"\\bdr\\b\",r\"doctor\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\bpeper\\b\",r\"pepper\",regex=True)\n",
    "# in a can, in can can they are all the same\n",
    "    sentences = sentences.str.replace(r\"\\bin\\s(1\\s)?can\\b\",\"can\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\bin\\s(1\\s)?bottle\\b\",\"bottle\",regex=True)\n",
    "# volume quantities : 200-milliliter -> i want it 200 ml\n",
    "    sentences = sentences.str.replace(r\"\\b([0-9]+)\\s*-\\s*(\\w+)\\b\",r\"\\1 \\2\",regex=True)\n",
    "# we may add more Normalization Techniques or delete some \"WHO KNOWS\" \n",
    "#TO DO: \n",
    "# (I think BBQ topping needs to be paired with things, it's always written as bbq_chicken, bbq_sauce, bbq_pulled_pork...)\n",
    "# i think this is oversimplification and i will let the sequence model decide this\n",
    "# To be decided later\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "1f1c145790e9de15",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SET of vocab and , SERIES of list of tokens\n",
    "def tokenization(sentences):\n",
    "    # merge the whole series int one sentence to make the vocab extracting faster\n",
    "    all_words = ' '.join(sentences)\n",
    "    # split on \\s\n",
    "    all_words = all_words.split(r\" \")\n",
    "    # keep the unique \n",
    "    vocab = set(all_words)\n",
    "    # i want to tokenize things like I'd to I , 'd\n",
    "    # new tokens that will come from I'd, it's ,....\n",
    "    new_tokens = set()\n",
    "    for word in vocab:\n",
    "        temp2 = word.split(\"'\")\n",
    "        # to make sure the 2nd token has its apostrophe: 'd (it should be two splitted words)\n",
    "        temp2[-1] = \"'\"+temp2[-1]\n",
    "        new_tokens.update(temp2)\n",
    "    vocab.update(new_tokens)\n",
    "    # we use expand to split the series into Dataframe (I think this will accelerate when i try to map the word into other thing)\n",
    "    pattern = r\"\\b([a-z]*)'([a-z]*)\\b\"\n",
    "    sentences = sentences.str.replace(pattern, r\"\\1 '\\2\",regex=True)\n",
    "    sentences = sentences.str.split(\" \",expand=True)\n",
    "    sentences.fillna(\"\",inplace=True)\n",
    "   # negation check regex : \\b(?<=not?)(.*?)(?=(\\.|,|$|and))\\b (for the future maybe ?)\n",
    "    return vocab, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d5cbb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pizza_drinks(parsed_tree): # the tree is a SERIES of format that is like this (ORDER (DRINK,))....\n",
    "# i extract PIZZAORDER node if exist, and DRINKORDER node if exist\n",
    "    pizza_orders, drink_orders = None, None\n",
    "    # remove the (ORDER and it's closing parenthesis at the end to ease next step\n",
    "    order_pattern = r\"(?<=\\(ORDER)(.*)(?=\\))\"\n",
    "    # this regex leads to 2 capture groups : anything after PIZZAORDER and before ) and anything after DRINKORDER and before )  \n",
    "    pizza_drink_order_patterns = r\" (?:\\(PIZZAORDER\\s*((?:\\([^\\)]+\\)\\s*)*)\\)\\s*)?(?:\\(DRINKORDER\\s*((?:\\([^\\)]+\\)\\s*)*)\\))? \"\n",
    "# match non capturing group (PIZZA ORDER someshit) if exist, and match non capturing group (DRINKORDER someshit) if exist\n",
    "# why non capturing? because i don't want the PD.extract to put it in the resulted Dataframe\n",
    "# in each group : search for (PIZZAORDER then space 0 more -it should be 1- then match \"(\" then\n",
    "# anything that isn't \")\" one or more -words- then space 0 or more then ) then space 0 or more IF EXIST same for DRINK\n",
    "    extracted_orders = parsed_tree.str.extract(order_pattern).iloc[:,0].str.strip()\n",
    "    extracted_PIZZA_DRINK = parsed_tree.str.extract(pizza_drink_order_patterns)\n",
    "    drink_orders = extracted_PIZZA_DRINK[1]\n",
    "    pizza_orders = extracted_PIZZA_DRINK[0]\n",
    "    drink_orders = drink_orders.dropna().reset_index(drop=True)\n",
    "    pizza_orders = pizza_orders.dropna().reset_index(drop=True)\n",
    "    del extracted_orders\n",
    "    del extracted_PIZZA_DRINK\n",
    "    return pizza_orders, drink_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "c3f36f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(pizza_orders,drink_orders):\n",
    "    pizza_nodes = []\n",
    "    pizza_node_attributes = [\"NUMBER\",\"SIZE\",\"TOPPING\",\"QUANTITY\"]\n",
    "    for attribute in pizza_node_attributes:\n",
    "        node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "        pizza_nodes.append(pizza_orders.str.extract(node_pattern))\n",
    "    drink_node_attributes = [\"NUMBER\",\"SIZE\",\"DRINKTYPE\",\"CONTAINERTYPE\",\"VOLUME\"]\n",
    "    drink_nodes = []\n",
    "    for attribute in drink_node_attributes:\n",
    "        node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "        drink_nodes.append(drink_orders.str.extract(node_pattern))\n",
    "    return pizza_nodes, drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6a312e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_nodes(pizza_nodes, drink_nodes):\n",
    "    # i want to refine the extracted nodes since the one parsed from previous step has\n",
    "    # alot of nans so i will drop those, normalize the text and drop the duplicates\n",
    "    # after this step i can start labling the text\n",
    "    new_pizza_nodes, new_drink_nodes = [], []\n",
    "    for i in range(0,4):\n",
    "        node = pizza_nodes[i].dropna().reset_index(drop=True)\n",
    "        if i < 2: # for size and number\n",
    "            node = pd.concat([node,drink_nodes[i].dropna().reset_index(drop=True)],axis =0, ignore_index=True)\n",
    "        node = node.iloc[:,0]\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = text_normalization(node)\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = node.reset_index(drop=True)\n",
    "        node = node.str.strip()\n",
    "        new_pizza_nodes.append(node)\n",
    "    for i in range(2,5):\n",
    "        node = drink_nodes[i].dropna().reset_index(drop=True)\n",
    "        node = node.iloc[:,0]\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = text_normalization(node)\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = node.reset_index(drop=True)\n",
    "        node = node.str.strip()\n",
    "        new_drink_nodes.append(node)\n",
    "    return new_pizza_nodes, new_drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "665a93f040d825db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:57.812078Z",
     "start_time": "2024-11-29T16:18:23.954568Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "cbfa8b23bbfc2bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:19:45.717518Z",
     "start_time": "2024-11-29T16:18:57.815096Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "now we talk about the data preprocessing\n",
    "What we should take into consideration? \n",
    "1- Word Normalization\n",
    "2- Word Tokenization\n",
    "Why we won't use Sentence segmentation? It's useless, orders are one sentence question no clear punctuation exist\n",
    "'''\n",
    "# NORMALIZATION\n",
    "normalized_sentence = text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "94bf7297650539b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-29T16:26:44.196520Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "# INITIAL tokenization we may need better implementations\n",
    "vocab, tokenized_sentence = tokenization(normalized_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ee32218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentence.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "fbe9a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenized_sentence\n",
    "del vocab_as_series\n",
    "del normalized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_orders, drink_orders = extract_pizza_drinks(decoupled_structured_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_nodes, drink_nodes = extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = clean_extracted_nodes(pizza_nodes, drink_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d06c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfold the list\n",
    "number, size, topping , quantity = pizza_nodes\n",
    "drink_type, container_type, volume = drink_nodes\n",
    "del pizza_nodes\n",
    "del drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dddc67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
