{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.945306Z",
     "start_time": "2024-11-29T16:16:11.593866Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "import nltk.stem\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525e1e8c84335bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.951944Z",
     "start_time": "2024-11-29T16:16:15.947321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d825ea706bbc1c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.960799Z",
     "start_time": "2024-11-29T16:16:15.953961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Dataset given path to JSON file\n",
    "# input: JSON file   -> output: list of size 4 (sentence, EXR, TOP, TOP_DECOUPLED) * number of strings\n",
    "# we will use this to read the Training/ evaluation / test datasets. \n",
    "def read_dataset(path: str):\n",
    "    data = pd.read_json(path, lines = True)\n",
    "    columns = data.columns.tolist()\n",
    "    parsed_json = [None]*len(columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        parsed_json[i] = data[columns[i]] # IDK will it be easier to us to work with pandas or numpy\n",
    "    return parsed_json # we store data in list of PD.Series for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974825debeb08f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.969130Z",
     "start_time": "2024-11-29T16:16:15.962816Z"
    }
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SERIES of String sentences\n",
    "def text_normalization(sentences):\n",
    "# Words to Lower\n",
    "    sentences = sentences.str.lower()\n",
    "# SIZES\n",
    "# after asking the TA, stating one format isn't a good idea so i won't standaradize the format\n",
    "# so things like Party - size , party size to standaradize this i will just remove the '-'\n",
    "    sentences = sentences.str.replace(r\"-\",\" \",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\s{2}\",r\" \",regex=True)\n",
    "# sometimes they refer to pizza as pie : WE ONLY SELL PIZZA\n",
    "    sentences = sentences.str.replace(\"pie\", \"pizza\")\n",
    "\n",
    "# Gluten - free we can leave it like that for now (may standardize it to gluten_free in future)\n",
    "\n",
    "# Quantities\n",
    "    '''\n",
    "    Now i want to take into consider quantities that means less of topping\n",
    "    something like \n",
    "    not much, not many, a little bit of, just a tiny bit of, just a bit, only a little, just a little, a bit, a little\n",
    "    we will leave those quantities like that for now and see in the future if we will change them\n",
    "    for quantities that mean much:\n",
    "    a lot of, lots of\n",
    "    '''\n",
    "\n",
    "# Quantity like \"a\" pizza should be converted to \"one\" , only one, just one -> one\n",
    "    sentences = sentences.str.replace(r\"\\ban?\\b\",\"one\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\b(?:only|just)\\sone\\b\",\"one\",regex=True)\n",
    "    \n",
    "# there are alot of Quantitative items 3 pies, Three pies ..\n",
    "# normalize digits to words \n",
    "    sentences = sentences.str.replace(r\"\\b([0-9]+)\\b\",lambda match: num2words(int(match.group(1))),regex=True)\n",
    "    \n",
    "# Negation\n",
    "    '''\n",
    "    There is multiple ways of negation, what i found while searching:\n",
    "    Without, hold the, With no(t), no, avoid\n",
    "    i want complex words like (hold the , without) to be converted int no\n",
    "    we won't change those for now because i want to try learn the context of negation\n",
    "    '''\n",
    "# TOPPINGS \n",
    "# (I think BBQ topping needs to be paired with things, it's always written as bbq_chicken, bbq_sauce, bbq_pulled_pork...)\n",
    "# i think this is oversimplification and i will let the sequence model decide this\n",
    "# To be decided later\n",
    "\n",
    "# DRINKS\n",
    "# sometimes people say pepsi, sometimes pepsis so i don't want plurals -> let's stem\n",
    "    sentences = sentences.str.replace(r\"\\b(\\w\\w+)e?s\\b\",r\"\\1\",regex=True)\n",
    "# sometimes san pellegrino is said pellegrino only\n",
    "    sentences = sentences.str.replace(r\"\\bsan\\s(pellegrino)\\b\",r\"\\1\",regex=True)\n",
    "# sometimes wrote zeros as zeroe\n",
    "    sentences = sentences.str.replace(r\"\\b(zero)e\\b\",r\"\\1\",regex=True)\n",
    "# sometimes people write iced instead of ice\n",
    "    sentences = sentences.str.replace(r\"\\b(ice)d\\b\",r\"\\1\",regex=True)\n",
    "# DOCTOR PEPPER convert dr to doctor , peper to pepper\n",
    "    sentences = sentences.str.replace(r\"\\bdr\\b\",r\"doctor\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\bpeper\\b\",r\"pepper\",regex=True)\n",
    "# stemmers we use Snowball stemmer\n",
    "    snowball = \n",
    "    sentences = sentences.apply()\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1f1c145790e9de15",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SET of vocab and , SERIES of list of tokens\n",
    "def tokenization(sentences):\n",
    "    # merge the whole series int one sentence to make the vocab extracting faster\n",
    "    all_words = ' '.join(sentences)\n",
    "    # split on \\s\n",
    "    all_words = all_words.split(r\" \")\n",
    "    # keep the unique \n",
    "    vocab = set(all_words)\n",
    "    # i want to tokenize things like I'd to I , 'd\n",
    "    # new tokens that will come from I'd, it's ,....\n",
    "    new_tokens = set()\n",
    "    for word in vocab:\n",
    "        temp2 = word.split(\"'\")\n",
    "        # to make sure the 2nd token has its apostrophe: 'd (it should be two splitted words)\n",
    "        temp2[-1] = \"'\"+temp2[-1]\n",
    "        new_tokens.update(temp2)\n",
    "    vocab.update(new_tokens)\n",
    "    # we use expand to split the series into Dataframe (I think this will accelerate when i try to map the word into other thing)\n",
    "    pattern = r\"\\b([a-z]*)'([a-z]*)\\b\"\n",
    "    sentences = sentences.str.replace(pattern, r\"\\1 '\\2\",regex=True)\n",
    "    sentences = sentences.str.split(\" \",expand=True)\n",
    "    sentences.fillna(\"\",inplace=True)\n",
    "   # negation check regex : \\b(?<=not?)(.*?)(?=(\\.|,|$|and))\\b (for the future maybe ?)\n",
    "    return vocab, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d5cbb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pizza_drinks(parsed_tree): # the tree is a SERIES of format that is like this (ORDER (DRINK,))....\n",
    "# i extract PIZZAORDER node if exist, and DRINKORDER node if exist\n",
    "    pizza_orders, drink_orders = None, None\n",
    "    # remove the (ORDER and it's closing parenthesis at the end to ease next step\n",
    "    order_pattern = r\"(?<=\\(ORDER)(.*)(?=\\))\"\n",
    "    # this regex leads to 2 capture groups : anything after PIZZAORDER and before ) and anything after DRINKORDER and before )  \n",
    "    pizza_drink_order_patterns = r\" (?:\\(PIZZAORDER\\s*((?:\\([^\\)]+\\)\\s*)*)\\)\\s*)?(?:\\(DRINKORDER\\s*((?:\\([^\\)]+\\)\\s*)*)\\))? \"\n",
    "# match non capturing group (PIZZA ORDER someshit) if exist, and match non capturing group (DRINKORDER someshit) if exist\n",
    "# why non capturing? because i don't want the PD.extract to put it in the resulted Dataframe\n",
    "# in each group : search for (PIZZAORDER then space 0 more -it should be 1- then match \"(\" then\n",
    "# anything that isn't \")\" one or more -words- then space 0 or more then ) then space 0 or more IF EXIST same for DRINK\n",
    "    extracted_orders = parsed_tree.str.extractall(order_pattern).iloc[:,0].str.strip()\n",
    "    extracted_PIZZA_DRINK = parsed_tree.str.extractall(pizza_drink_order_patterns)\n",
    "    drink_orders = extracted_PIZZA_DRINK[1]\n",
    "    pizza_orders = extracted_PIZZA_DRINK[0]\n",
    "    drink_orders = drink_orders.dropna().reset_index(drop=True)\n",
    "    pizza_orders = pizza_orders.dropna().reset_index(drop=True)\n",
    "    del extracted_orders\n",
    "    del extracted_PIZZA_DRINK\n",
    "    return pizza_orders, drink_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c3f36f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(pizza_orders,drink_orders):\n",
    "    drink_nodes, pizza_nodes = [] ,[]\n",
    "    if np.any(pizza_orders) :\n",
    "        pizza_node_attributes = [\"NUMBER\",\"SIZE\",\"TOPPING\",\"QUANTITY\",\"STYLE\"]\n",
    "        for attribute in pizza_node_attributes:\n",
    "            node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "            pizza_nodes.append(pizza_orders.str.extract(node_pattern))\n",
    "            \n",
    "    if np.any(drink_orders) :\n",
    "        drink_node_attributes = [\"NUMBER\",\"SIZE\",\"DRINKTYPE\",\"CONTAINERTYPE\",\"VOLUME\"]\n",
    "        for attribute in drink_node_attributes:\n",
    "            node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "            drink_nodes.append(drink_orders.str.extract(node_pattern))\n",
    "    return pizza_nodes, drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6a312e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_nodes(pizza_nodes, drink_nodes):\n",
    "    # i want to refine the extracted nodes since the one parsed from previous step has\n",
    "    # alot of nans so i will drop those, normalize the text and drop the duplicates\n",
    "    # after this step i can start labling the text\n",
    "    new_pizza_nodes, new_drink_nodes = [], []\n",
    "    for i in range(0,5):\n",
    "        node = pizza_nodes[i].dropna().reset_index(drop=True)\n",
    "        if i < 2: # for size and number\n",
    "            node = pd.concat([node,drink_nodes[i].dropna().reset_index(drop=True)],axis =0, ignore_index=True)\n",
    "        node = node.iloc[:,0]\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = text_normalization(node)\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = node.reset_index(drop=True)\n",
    "        node = node.str.strip()\n",
    "        new_pizza_nodes.append(node)\n",
    "    for i in range(2,5):\n",
    "        node = drink_nodes[i].dropna().reset_index(drop=True)\n",
    "        node = node.iloc[:,0]\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = text_normalization(node)\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = node.reset_index(drop=True)\n",
    "        node = node.str.strip()\n",
    "        new_drink_nodes.append(node)\n",
    "    return new_pizza_nodes, new_drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "665a93f040d825db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:57.812078Z",
     "start_time": "2024-11-29T16:18:23.954568Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentences, parsed_tree, structured_sentence, decoupled_structured_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./PIZZA_train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m, in \u001b[0;36mread_dataset\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_dataset\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      7\u001b[0m     parsed_json \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(columns)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:905\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m--> 905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:916\u001b[0m, in \u001b[0;36mJsonReader._preprocess_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03mAt this point, the data either has a `read` attribute (e.g. a file\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03mobject or a StringIO) or is a string that is a JSON document.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;124;03mOtherwise, we read it into memory for the `read` method.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows):\n\u001b[1;32m--> 916\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1116\u001b[0m, in \u001b[0;36mJsonReader.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1118\u001b[0m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1119\u001b[0m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1120\u001b[0m     traceback: TracebackType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1121\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "cbfa8b23bbfc2bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:19:45.717518Z",
     "start_time": "2024-11-29T16:18:57.815096Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "now we talk about the data preprocessing\n",
    "What we should take into consideration? \n",
    "1- Word Normalization\n",
    "2- Word Tokenization\n",
    "Why we won't use Sentence segmentation? It's useless, orders are one sentence question no clear punctuation exist\n",
    "'''\n",
    "# NORMALIZATION\n",
    "normalized_sentence = text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "94bf7297650539b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-29T16:26:44.196520Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "# INITIAL tokenization we may need better implementations\n",
    "vocab, tokenized_sentence = tokenization(normalized_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ee32218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentence.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "fbe9a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenized_sentence\n",
    "del vocab_as_series\n",
    "del normalized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1d1f83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_orders, drink_orders = extract_pizza_drinks(decoupled_structured_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1d9d83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_nodes, drink_nodes = extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = clean_extracted_nodes(pizza_nodes, drink_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c3d06c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfold the list\n",
    "number, size, topping , quantity, style = pizza_nodes\n",
    "drink_type, container_type, volume = drink_nodes\n",
    "del pizza_nodes\n",
    "del drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dfb95d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               large\n",
      "1          party size\n",
      "2      personal sized\n",
      "3             regular\n",
      "4         party sized\n",
      "5        party  sized\n",
      "6        lunch  sized\n",
      "7          lunch size\n",
      "8            personal\n",
      "9         party  size\n",
      "10      personal size\n",
      "11             medium\n",
      "12              small\n",
      "13        lunch sized\n",
      "14    personal  sized\n",
      "15        lunch  size\n",
      "16     personal  size\n",
      "17        extra large\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0e7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'regular'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordstem2 = nltk.stem.PorterStemmer(\"english\")\n",
    "wordstem = nltk.stem.SnowballStemmer(\"english\")\n",
    "wordstem.stem(\"regular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095daf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
