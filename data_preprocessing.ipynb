{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.945306Z",
     "start_time": "2024-11-29T16:16:11.593866Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "import nltk.stem\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525e1e8c84335bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.951944Z",
     "start_time": "2024-11-29T16:16:15.947321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d825ea706bbc1c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.960799Z",
     "start_time": "2024-11-29T16:16:15.953961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Dataset given path to JSON file\n",
    "# input: JSON file   -> output: list of size 4 (sentence, EXR, TOP, TOP_DECOUPLED) * number of strings\n",
    "# we will use this to read the Training/ evaluation / test datasets. \n",
    "def read_dataset(path: str):\n",
    "    data = pd.read_json(path, lines = True)\n",
    "    columns = data.columns.tolist()\n",
    "    parsed_json = [None]*len(columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        parsed_json[i] = data[columns[i]] # IDK will it be easier to us to work with pandas or numpy\n",
    "    return parsed_json # we store data in list of PD.Series for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974825debeb08f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.969130Z",
     "start_time": "2024-11-29T16:16:15.962816Z"
    }
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SERIES of String sentences\n",
    "def text_normalization(sentences):\n",
    "# Words to Lower\n",
    "    sentences = sentences.str.lower()\n",
    "# SIZES\n",
    "# after asking the TA, stating one format isn't a good idea so i won't standaradize the format\n",
    "# so things like Party - size , party size to standaradize this i will just remove the '-'\n",
    "    sentences = sentences.str.replace(r\"-\",\" \",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\s{2}\",r\" \",regex=True)\n",
    "# sometimes they refer to pizza as pie : WE ONLY SELL PIZZA\n",
    "    sentences = sentences.str.replace(\"pie\", \"pizza\")\n",
    "\n",
    "# Gluten - free we can leave it like that for now (may standardize it to gluten_free in future)\n",
    "\n",
    "# Quantities\n",
    "    '''\n",
    "    Now i want to take into consider quantities that means less of topping\n",
    "    something like \n",
    "    not much, not many, a little bit of, just a tiny bit of, just a bit, only a little, just a little, a bit, a little\n",
    "    we will leave those quantities like that for now and see in the future if we will change them\n",
    "    for quantities that mean much:\n",
    "    a lot of, lots of\n",
    "    '''\n",
    "\n",
    "# Quantity like \"a\" pizza should be converted to \"one\" , only one, just one -> one\n",
    "    sentences = sentences.str.replace(r\"\\ban?\\b\",\"one\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\b(?:only|just)\\sone\\b\",\"one\",regex=True)\n",
    "    \n",
    "# there are alot of Quantitative items 3 pies, Three pies ..\n",
    "# normalize digits to words \n",
    "    sentences = sentences.str.replace(r\"\\b([0-9]+)\\b\",lambda match: num2words(int(match.group(1))),regex=True)\n",
    "    \n",
    "# Negation\n",
    "    '''\n",
    "    There is multiple ways of negation, what i found while searching:\n",
    "    Without, hold the, With no(t), no, avoid\n",
    "    i want complex words like (hold the , without) to be converted int no\n",
    "    we won't change those for now because i want to try learn the context of negation\n",
    "    '''\n",
    "# TOPPINGS \n",
    "# (I think BBQ topping needs to be paired with things, it's always written as bbq_chicken, bbq_sauce, bbq_pulled_pork...)\n",
    "# i think this is oversimplification and i will let the sequence model decide this\n",
    "# To be decided later\n",
    "\n",
    "# DRINKS\n",
    "# sometimes people say pepsi, sometimes pepsis so i don't want plurals -> let's stem\n",
    "    sentences = sentences.str.replace(r\"\\b(\\w\\w+)e?s\\b\",r\"\\1\",regex=True)\n",
    "# sometimes san pellegrino is said pellegrino only\n",
    "    sentences = sentences.str.replace(r\"\\bsan\\s(pellegrino)\\b\",r\"\\1\",regex=True)\n",
    "# sometimes wrote zeros as zeroe\n",
    "    sentences = sentences.str.replace(r\"\\b(zero)e\\b\",r\"\\1\",regex=True)\n",
    "# sometimes people write iced instead of ice\n",
    "    sentences = sentences.str.replace(r\"\\b(ice)d\\b\",r\"\\1\",regex=True)\n",
    "# DOCTOR PEPPER convert dr to doctor , peper to pepper\n",
    "    sentences = sentences.str.replace(r\"\\bdr\\b\",r\"doctor\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\bpeper\\b\",r\"pepper\",regex=True)\n",
    "# stemmers we use Snowball stemmer\n",
    "\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f1c145790e9de15",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SET of vocab and , SERIES of list of tokens\n",
    "def tokenization(sentences):\n",
    "    # merge the whole series int one sentence to make the vocab extracting faster\n",
    "    all_words = ' '.join(sentences)\n",
    "    # split on \\s\n",
    "    all_words = all_words.split(r\" \")\n",
    "    # keep the unique \n",
    "    vocab = set(all_words)\n",
    "    # i want to tokenize things like I'd to I , 'd\n",
    "    # new tokens that will come from I'd, it's ,....\n",
    "    new_tokens = set()\n",
    "    for word in vocab:\n",
    "        temp2 = word.split(\"'\")\n",
    "        # to make sure the 2nd token has its apostrophe: 'd (it should be two splitted words)\n",
    "        temp2[-1] = \"'\"+temp2[-1]\n",
    "        new_tokens.update(temp2)\n",
    "    vocab.update(new_tokens)\n",
    "    # we use expand to split the series into Dataframe (I think this will accelerate when i try to map the word into other thing)\n",
    "    pattern = r\"\\b([a-z]*)'([a-z]*)\\b\"\n",
    "    sentences = sentences.str.replace(pattern, r\"\\1 '\\2\",regex=True)\n",
    "    sentences = sentences.str.split(\" \",expand=True)\n",
    "    sentences.fillna(\"\",inplace=True)\n",
    "   # negation check regex : \\b(?<=not?)(.*?)(?=(\\.|,|$|and))\\b (for the future maybe ?)\n",
    "    return vocab, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5cbb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pizza_drinks(parsed_tree): # the tree is a SERIES of format that is like this (ORDER (DRINK,))....\n",
    "# i extract PIZZAORDER node if exist, and DRINKORDER node if exist\n",
    "    pizza_orders, drink_orders = None, None\n",
    "    # remove the (ORDER and it's closing parenthesis at the end to ease next step\n",
    "    order_pattern = r\"(?<=\\(ORDER)(.*)(?=\\))\"\n",
    "    # this regex leads to 2 capture groups : anything after PIZZAORDER and before ) and anything after DRINKORDER and before )  \n",
    "    pizza_drink_order_patterns = r\" (?:\\(PIZZAORDER\\s*((?:\\([^\\)]+\\)\\s*)*)\\)\\s*)?(?:\\(DRINKORDER\\s*((?:\\([^\\)]+\\)\\s*)*)\\))? \"\n",
    "# match non capturing group (PIZZA ORDER someshit) if exist, and match non capturing group (DRINKORDER someshit) if exist\n",
    "# why non capturing? because i don't want the PD.extract to put it in the resulted Dataframe\n",
    "# in each group : search for (PIZZAORDER then space 0 more -it should be 1- then match \"(\" then\n",
    "# anything that isn't \")\" one or more -words- then space 0 or more then ) then space 0 or more IF EXIST same for DRINK\n",
    "    extracted_orders = parsed_tree.str.extractall(order_pattern).iloc[:,0].str.strip()\n",
    "    extracted_PIZZA_DRINK = parsed_tree.str.extractall(pizza_drink_order_patterns)\n",
    "    drink_orders = extracted_PIZZA_DRINK[1]\n",
    "    pizza_orders = extracted_PIZZA_DRINK[0]\n",
    "    drink_orders = drink_orders.dropna().reset_index(drop=True)\n",
    "    pizza_orders = pizza_orders.dropna().reset_index(drop=True)\n",
    "    del extracted_orders\n",
    "    del extracted_PIZZA_DRINK\n",
    "    return pizza_orders, drink_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f36f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(pizza_orders,drink_orders):\n",
    "    drink_nodes, pizza_nodes = [] ,[]\n",
    "    if np.any(pizza_orders) :\n",
    "        pizza_node_attributes = [\"NUMBER\",\"SIZE\",\"TOPPING\",\"QUANTITY\",\"STYLE\"]\n",
    "        for attribute in pizza_node_attributes:\n",
    "            node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "            pizza_nodes.append(pizza_orders.str.extract(node_pattern))\n",
    "            \n",
    "    if np.any(drink_orders) :\n",
    "        drink_node_attributes = [\"NUMBER\",\"SIZE\",\"DRINKTYPE\",\"CONTAINERTYPE\",\"VOLUME\"]\n",
    "        for attribute in drink_node_attributes:\n",
    "            node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "            drink_nodes.append(drink_orders.str.extract(node_pattern))\n",
    "    return pizza_nodes, drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a312e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_nodes(pizza_nodes, drink_nodes):\n",
    "    # i want to refine the extracted nodes since the one parsed from previous step has\n",
    "    # alot of nans so i will drop those, normalize the text and drop the duplicates\n",
    "    # after this step i can start labling the text\n",
    "    new_pizza_nodes, new_drink_nodes = [], []\n",
    "    for i in range(0,5):\n",
    "        node = pizza_nodes[i].dropna().reset_index(drop=True)\n",
    "        if i < 2: # for size and number\n",
    "            node = pd.concat([node,drink_nodes[i].dropna().reset_index(drop=True)],axis =0, ignore_index=True)\n",
    "        node = node.iloc[:,0]\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = text_normalization(node)\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = node.reset_index(drop=True)\n",
    "        node = node.str.strip()\n",
    "        new_pizza_nodes.append(node)\n",
    "    for i in range(2,5):\n",
    "        node = drink_nodes[i].dropna().reset_index(drop=True)\n",
    "        node = node.iloc[:,0]\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = text_normalization(node)\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = node.reset_index(drop=True)\n",
    "        node = node.str.strip()\n",
    "        new_drink_nodes.append(node)\n",
    "    return new_pizza_nodes, new_drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "665a93f040d825db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:57.812078Z",
     "start_time": "2024-11-29T16:18:23.954568Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "cbfa8b23bbfc2bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:19:45.717518Z",
     "start_time": "2024-11-29T16:18:57.815096Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "now we talk about the data preprocessing\n",
    "What we should take into consideration? \n",
    "1- Word Normalization\n",
    "2- Word Tokenization\n",
    "Why we won't use Sentence segmentation? It's useless, orders are one sentence question no clear punctuation exist\n",
    "'''\n",
    "# NORMALIZATION\n",
    "normalized_sentence = text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "94bf7297650539b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-29T16:26:44.196520Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "# INITIAL tokenization we may need better implementations\n",
    "vocab, tokenized_sentence = tokenization(normalized_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ee32218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentence.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "fbe9a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenized_sentence\n",
    "del vocab_as_series\n",
    "del normalized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d1f83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_orders, drink_orders = extract_pizza_drinks(decoupled_structured_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d9d83f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Series.apply() missing 1 required positional argument: 'func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pizza_nodes, drink_nodes \u001b[38;5;241m=\u001b[39m extract_nodes(pizza_orders,drink_orders)\n\u001b[1;32m----> 2\u001b[0m pizza_nodes, drink_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mclean_extracted_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpizza_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrink_nodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mclean_extracted_nodes\u001b[1;34m(pizza_nodes, drink_nodes)\u001b[0m\n\u001b[0;32m     10\u001b[0m node \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m node\u001b[38;5;241m.\u001b[39mdrop_duplicates(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[43mtext_normalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m node\u001b[38;5;241m.\u001b[39mdrop_duplicates(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m node \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[12], line 59\u001b[0m, in \u001b[0;36mtext_normalization\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     56\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sentences\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbpeper\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpepper\u001b[39m\u001b[38;5;124m\"\u001b[39m,regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# stemmers we use Snowball stemmer\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msentences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences\n",
      "\u001b[1;31mTypeError\u001b[0m: Series.apply() missing 1 required positional argument: 'func'"
     ]
    }
   ],
   "source": [
    "pizza_nodes, drink_nodes = extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = clean_extracted_nodes(pizza_nodes, drink_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d06c98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pizza_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# unfold the list\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m number, size, topping , quantity, style \u001b[38;5;241m=\u001b[39m \u001b[43mpizza_nodes\u001b[49m\n\u001b[0;32m      3\u001b[0m drink_type, container_type, volume \u001b[38;5;241m=\u001b[39m drink_nodes\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pizza_nodes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pizza_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "# unfold the list\n",
    "number, size, topping , quantity, style = pizza_nodes\n",
    "drink_type, container_type, volume = drink_nodes\n",
    "del pizza_nodes\n",
    "del drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dfb95d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               large\n",
      "1          party size\n",
      "2      personal sized\n",
      "3             regular\n",
      "4         party sized\n",
      "5        party  sized\n",
      "6        lunch  sized\n",
      "7          lunch size\n",
      "8            personal\n",
      "9         party  size\n",
      "10      personal size\n",
      "11             medium\n",
      "12              small\n",
      "13        lunch sized\n",
      "14    personal  sized\n",
      "15        lunch  size\n",
      "16     personal  size\n",
      "17        extra large\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "96d0e7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iced tea\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'size'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordstem2 = nltk.stem.PorterStemmer()\n",
    "wordstem = nltk.stem.SnowballStemmer(\"english\")\n",
    "print(wordstem.stem(\"iced tea\"))\n",
    "wordstem2.stem(\"sized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095daf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
