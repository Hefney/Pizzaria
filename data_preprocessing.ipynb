{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.945306Z",
     "start_time": "2024-11-29T16:16:11.593866Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525e1e8c84335bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.951944Z",
     "start_time": "2024-11-29T16:16:15.947321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d825ea706bbc1c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.960799Z",
     "start_time": "2024-11-29T16:16:15.953961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Dataset given path to JSON file\n",
    "# input: JSON file   -> output: list of size 4 (sentence, EXR, TOP, TOP_DECOUPLED) * number of strings\n",
    "# we will use this to read the Training/ evaluation / test datasets. \n",
    "def read_dataset(path: str):\n",
    "    data = pd.read_json(path, lines = True)\n",
    "    columns = data.columns.tolist()\n",
    "    parsed_json = [None]*len(columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        parsed_json[i] = data[columns[i]] # IDK will it be easier to us to work with pandas or numpy\n",
    "    return parsed_json # we store data in list of PD.Series for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9974825debeb08f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.969130Z",
     "start_time": "2024-11-29T16:16:15.962816Z"
    }
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SERIES of String sentences\n",
    "def text_normalization(sentences):\n",
    "# convert words to lower\n",
    "    sentences = sentences.str.lower()\n",
    "# sizes are flexible some are party-sized, party size , lunch - sized ....\n",
    "# i will assume one format A-B : party-sized\n",
    "# now we build our regex where we search for \"(some shit) size\" and convert it to some shit-size \n",
    "    pattern_size =r'''(?xm) # multiline verbose flag to enable comments\n",
    "        \\b([a-z]*)[-\\s]*(size)(?:d)?\\b # i search for alphabets that is followed by one or more spaces or \"-\" then one or more spaces\n",
    "                                       # then size, not capturing d because i don't want it anymore (i don't want it in group actually)\n",
    "        '''\n",
    "    sentences = sentences.str.replace(pattern_size, r\"\\1-\\2\",regex=True)\n",
    "# sometimes they ask the pizza to be gluten-free, gluten free -> normalizing this to gluten-free\n",
    "    pattern_free = r'''(?xm) \n",
    "        \\b([a-z]*)[-\\s]*(free)\\b\n",
    "        '''\n",
    "    sentences = sentences.str.replace(pattern_free, r\"\\1-\\2\",regex=True)\n",
    "# sometimes they refer to pizza as pie : WE ONLY SELL PIZZA\n",
    "    sentences = sentences.str.replace(\"pie\", \"pizza\")\n",
    "# there are alot of Quantitative items 3 pies, 250 ml, 552 ....\n",
    "# we can normalizing this to a quantity flag something like <Q>\n",
    "    pattern_digits = r\"[0-9]+\"\n",
    "# we make the flag as lowercase \"quta\" short for quantity \n",
    "    sentences = sentences.str.replace(pattern_digits, \"quta\", regex=True)\n",
    "#TO DO:\n",
    "# we still didn't normalize the number of food things like a pizza with , one pizza of\n",
    "# a and one represent same thing, Three , two should be normalized into quta flags \n",
    "\n",
    "# we may add more Normalization Techniques or delete some \"WHO KNOWS\"\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1c145790e9de15",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SET of vocab and , SERIES of list of tokens\n",
    "def tokenization(sentences):\n",
    "    # merge the whole series int one sentence to make the vocab extracting faster\n",
    "    all_words = ' '.join(sentences)\n",
    "    # split on \\s\n",
    "    all_words = all_words.split(r\" \")\n",
    "    # keep the unique \n",
    "    vocab = set(all_words)\n",
    "    # i want to tokenize things like I'd to I , 'd\n",
    "    # new tokens that will come from I'd, it's ,....\n",
    "    new_tokens = set()\n",
    "    for word in vocab:\n",
    "        temp2 = word.split(\"'\")\n",
    "        # to make sure the 2nd token has its apostrophe: 'd (it should be two splitted words)\n",
    "        temp2[-1] = \"'\"+temp2[-1]\n",
    "        new_tokens.update(temp2)\n",
    "    vocab.update(new_tokens)\n",
    "    # we can use EXPAND flag and convert series to dataframe (but who knows do we need to do that?)\n",
    "    pattern = r\"\\b([a-z]*)'([a-z]*)\\b\"\n",
    "    sentences = sentences.str.replace(pattern, r\"\\1 '\\2\",regex=True)\n",
    "    sentences = sentences.str.split(\" \")\n",
    "   \n",
    "    return vocab, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665a93f040d825db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:57.812078Z",
     "start_time": "2024-11-29T16:18:23.954568Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbfa8b23bbfc2bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:19:45.717518Z",
     "start_time": "2024-11-29T16:18:57.815096Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "now we talk about the data preprocessing\n",
    "What we should take into consideration? \n",
    "1- Word Normalization\n",
    "2- Word Tokenization\n",
    "Why we won't use Sentence segmentation? It's useless, orders are one sentence question no clear punctuation exist\n",
    "'''\n",
    "# NORMALIZATION\n",
    "normalized_sentence = text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94bf7297650539b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-29T16:26:44.196520Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "# INITIAL tokenization we may need better implementations\n",
    "vocab, tokenized_sentence = tokenization(normalized_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe4a20eccd0a7dc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:26:15.554930Z",
     "start_time": "2024-11-29T16:26:15.548452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560 0          [can, i, have, a, large, bbq, pulled, pork]\n",
      "1    [large, pizza, with, green, pepper, and, with,...\n",
      "2           [i, 'd, like, a, large, vegetarian, pizza]\n",
      "3    [party-size, stuffed, crust, pizza, with, amer...\n",
      "4        [can, i, have, one, personal-size, artichoke]\n",
      "Name: train.SRC, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab), tokenized_sentence.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63aab8726e5f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
