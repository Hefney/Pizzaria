{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.945306Z",
     "start_time": "2024-11-29T16:16:11.593866Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525e1e8c84335bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.951944Z",
     "start_time": "2024-11-29T16:16:15.947321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d825ea706bbc1c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.960799Z",
     "start_time": "2024-11-29T16:16:15.953961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Dataset given path to JSON file\n",
    "# input: JSON file   -> output: list of size 4 (sentence, EXR, TOP, TOP_DECOUPLED) * number of strings\n",
    "# we will use this to read the Training/ evaluation / test datasets. \n",
    "def read_dataset(path: str):\n",
    "    data = pd.read_json(path, lines = True)\n",
    "    columns = data.columns.tolist()\n",
    "    parsed_json = [None]*len(columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        parsed_json[i] = data[columns[i]] # IDK will it be easier to us to work with pandas or numpy\n",
    "    return parsed_json # we store data in list of PD.Series for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9974825debeb08f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.969130Z",
     "start_time": "2024-11-29T16:16:15.962816Z"
    }
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SERIES of String sentences\n",
    "def pre_text_normalization(sentences,flag=-1):\n",
    "# Words to Lower\n",
    "    if flag == -1:\n",
    "        sentences = sentences.str.lower()\n",
    "# SIZES\n",
    "# after asking the TA, stating one format isn't a good idea so i won't standaradize the format\n",
    "# so things like Party - size , party size to standaradize this i will just remove the '-'\n",
    "    sentences = sentences.str.replace(r\"-\",\" \",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\s{2}\",r\" \",regex=True)\n",
    "# sometimes they refer to pizza as pie : WE ONLY SELL PIZZA\n",
    "    sentences = sentences.str.replace(\"pie\", \"pizza\")\n",
    "\n",
    "# Gluten - free we can leave it like that for now (may standardize it to gluten_free in future)\n",
    "\n",
    "# Quantities\n",
    "    '''\n",
    "    Now i want to take into consider quantities that means less of topping\n",
    "    something like \n",
    "    not much, not many, a little bit of, just a tiny bit of, just a bit, only a little, just a little, a bit, a little\n",
    "    we will leave those quantities like that for now and see in the future if we will change them\n",
    "    for quantities that mean much:\n",
    "    a lot of, lots of\n",
    "    '''\n",
    "\n",
    "# Quantity like \"a\" pizza should be converted to \"one\" , only one, just one -> one\n",
    "    sentences = sentences.str.replace(r\"\\ban?(?!\\s+(bit|tiny|lot|little))\\b\",\"one\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\b(?:only|just)\\sone\\b\",\"one\",regex=True)\n",
    "    \n",
    "# there are alot of Quantitative items 3 pies, Three pies ..\n",
    "# normalize digits to words \n",
    "    sentences = sentences.str.replace(r\"\\b([0-9]+)\\b\",lambda match: num2words(int(match.group(1))),regex=True)\n",
    "    \n",
    "# Negation\n",
    "    '''\n",
    "    There is multiple ways of negation, what i found while searching:\n",
    "    Without, hold the, With no(t), no, avoid\n",
    "    i want complex words like (hold the , without) to be converted int no\n",
    "    we won't change those for now because i want to try learn the context of negation\n",
    "    '''\n",
    "# TOPPINGS \n",
    "# (I think BBQ topping needs to be paired with things, it's always written as bbq_chicken, bbq_sauce, bbq_pulled_pork...)\n",
    "# i think this is oversimplification and i will let the sequence model decide this\n",
    "# To be decided later\n",
    "\n",
    "# DRINKS\n",
    "# sometimes people say pepsi, sometimes pepsis so i don't want plurals -> let's stem\n",
    "    sentences = sentences.str.replace(r\"\\b(\\w\\w+)e?s\\b\",r\"\\1\",regex=True)\n",
    "# sometimes san pellegrino is said pellegrino only\n",
    "    sentences = sentences.str.replace(r\"\\bsan\\s(pellegrino)\\b\",r\"\\1\",regex=True)\n",
    "# sometimes wrote zeros as zeroe\n",
    "    sentences = sentences.str.replace(r\"\\b(zero)e\\b\",r\"\\1\",regex=True)\n",
    "# sometimes people write iced instead of ice\n",
    "    sentences = sentences.str.replace(r\"\\b(ice)d\\b\",r\"\\1\",regex=True)\n",
    "# DOCTOR PEPPER convert dr to doctor , peper to pepper\n",
    "    sentences = sentences.str.replace(r\"\\bdr\\b\",r\"doctor\",regex=True)\n",
    "    sentences = sentences.str.replace(r\"\\bpeper\\b\",r\"pepper\",regex=True)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01227774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer \n",
    "def snow_ball_stemmer(vocab):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    if isinstance(vocab,set):\n",
    "        vocab = set([stemmer.stem(word) for word in vocab])\n",
    "        return vocab\n",
    "    else:\n",
    "        vocab = vocab.apply(lambda words: [stemmer.stem(word) for word in words])\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1c145790e9de15",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SET of vocab and , SERIES of list of tokens\n",
    "def tokenization(sentences):\n",
    "    # merge the whole series int one sentence to make the vocab extracting faster\n",
    "    all_words = ' '.join(sentences)\n",
    "    # used penn treebank tokenizer\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    all_words = tokenizer.tokenize(all_words)\n",
    "    \n",
    "    # keep the unique \n",
    "    vocab = set(all_words)\n",
    "    # i want to tokenize things like I'd to I , 'd\n",
    "    # new tokens that will come from I'd, it's ,....\n",
    "    # something is wrong with apstrophe\n",
    "    \n",
    "    # we use expand to split the series into Dataframe (I think this will accelerate when i try to map the word into other thing)\n",
    "    pattern = r\"\\b([a-z]*)'([a-z]*)\\b\"\n",
    "    sentences = sentences.str.replace(pattern, r\"\\1 '\\2\",regex=True)\n",
    "    sentences = sentences.apply(tokenizer.tokenize)\n",
    "    sentences.fillna(\"\",inplace=True)\n",
    "   # negation check regex : \\b(?<=not?)(.*?)(?=(\\.|,|$|and))\\b (for the future maybe ?)\n",
    "    return vocab, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5cbb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pizza_drinks(parsed_tree): # the tree is a SERIES of format that is like this (ORDER (DRINK,))....\n",
    "# i extract PIZZAORDER node if exist, and DRINKORDER node if exist\n",
    "    pizza_orders, drink_orders = None, None\n",
    "    # remove the (ORDER and it's closing parenthesis at the end to ease next step\n",
    "    extracted_words_before_parsing = r\"(?:(?:\\(ORDER\\s+)|(?:\\)))([^()]+)(?=[\\s(]+)\"\n",
    "    none_words = parsed_tree.str.extractall(extracted_words_before_parsing).iloc[:,0].str.strip()\n",
    "    none_words = none_words.dropna().reset_index(drop=True)\n",
    "    order_pattern = r\"(?<=\\(ORDER)(.*)(?=\\))\"\n",
    "    # this regex leads to 2 capture groups : anything after PIZZAORDER and before ) and anything after DRINKORDER and before )  \n",
    "    pizza_drink_order_patterns = r\" (?:\\(PIZZAORDER\\s*((?:\\(?[^\\)]+\\)?\\s*)*)\\)\\s*)?(?:\\(DRINKORDER\\s*((?:\\(?[^\\)]+\\)?\\s*)*)\\)\\s*)? \"\n",
    "# match non capturing group (PIZZA ORDER someshit) if exist, and match non capturing group (DRINKORDER someshit) if exist\n",
    "# why non capturing? because i don't want the PD.extract to put it in the resulted Dataframe\n",
    "# in each group : search for (PIZZAORDER then space 0 more -it should be 1- then match \"(\" then\n",
    "# anything that isn't \")\" one or more -words- then space 0 or more then ) then space 0 or more IF EXIST same for DRINK\n",
    "    extracted_orders = parsed_tree.str.extractall(order_pattern).iloc[:,0].str.strip()\n",
    "    extracted_PIZZA_DRINK = parsed_tree.str.extractall(pizza_drink_order_patterns)\n",
    "    \n",
    "    drink_orders = extracted_PIZZA_DRINK[1]\n",
    "    pizza_orders = extracted_PIZZA_DRINK[0]\n",
    "    drink_orders = drink_orders.dropna().reset_index(drop=True)\n",
    "    pizza_orders = pizza_orders.dropna().reset_index(drop=True)\n",
    "\n",
    "    pizza_orders = pizza_orders.replace(r\"(?<=\\))(.*?)(?=\\()\",r\"(NONE \\1)\",regex=True)\n",
    "    pizza_orders = pizza_orders.replace(r\"\\(NONE\\s*\\)\", \"\",regex=True)\n",
    "    drink_orders = drink_orders.replace(r\"(?<=\\))(.*?)(?=\\()\",r\"(NONE \\1)\",regex=True)\n",
    "    drink_orders = drink_orders.replace(r\"\\(NONE\\s*\\)\", \"\",regex=True)\n",
    "    \n",
    "    del extracted_orders\n",
    "    del extracted_PIZZA_DRINK\n",
    "    return pizza_orders, drink_orders, none_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3f36f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(pizza_orders,drink_orders):\n",
    "    drink_nodes, pizza_nodes = [] ,[]\n",
    "    if np.any(pizza_orders) :\n",
    "        pizza_node_attributes = [\"NUMBER\",\"SIZE\",\"NONE\",\"TOPPING\",\"QUANTITY\",\"STYLE\"]\n",
    "        for attribute in pizza_node_attributes:\n",
    "            node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "            pizza_nodes.append(pizza_orders.str.extract(node_pattern))\n",
    "            \n",
    "    if np.any(drink_orders) :\n",
    "        drink_node_attributes = [\"NUMBER\",\"SIZE\",\"NONE\", \"DRINKTYPE\",\"CONTAINERTYPE\",\"VOLUME\"]\n",
    "        for attribute in drink_node_attributes:\n",
    "            node_pattern = r\"(?<=\\(\"+attribute+r\")(.*?)(?=\\))\"\n",
    "            drink_nodes.append(drink_orders.str.extract(node_pattern))\n",
    "    return pizza_nodes, drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a312e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_nodes(pizza_nodes, drink_nodes):\n",
    "    # i want to refine the extracted nodes since the one parsed from previous step has\n",
    "    # alot of nans so i will drop those, normalize the text and drop the duplicates\n",
    "    # after this step i can start labling the text\n",
    "    new_pizza_nodes, new_drink_nodes = [], []\n",
    "    for i in range(0,6):\n",
    "        node = pizza_nodes[i].dropna().reset_index(drop=True)\n",
    "        if i < 3: # for size and number and none\n",
    "            node = pd.concat([node,drink_nodes[i].dropna().reset_index(drop=True)],axis =0, ignore_index=True)\n",
    "        node = node.iloc[:,0]\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = pre_text_normalization(node)\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = node.reset_index(drop=True)\n",
    "        node = node.str.strip()\n",
    "        new_pizza_nodes.append(node)\n",
    "    for i in range(3,6):\n",
    "        node = drink_nodes[i].dropna().reset_index(drop=True)\n",
    "        node = node.iloc[:,0]\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = pre_text_normalization(node)\n",
    "        node.drop_duplicates(keep='first',inplace=True)\n",
    "        node = node.reset_index(drop=True)\n",
    "        node = node.str.strip()\n",
    "        new_drink_nodes.append(node)\n",
    "    return new_pizza_nodes, new_drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665a93f040d825db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:57.812078Z",
     "start_time": "2024-11-29T16:18:23.954568Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f167f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_sentence = pd.read_csv(\"./TOP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbfa8b23bbfc2bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:19:45.717518Z",
     "start_time": "2024-11-29T16:18:57.815096Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "now we talk about the data preprocessing\n",
    "What we should take into consideration? \n",
    "1- Word Normalization\n",
    "2- Word Tokenization\n",
    "Why we won't use Sentence segmentation? It's useless, orders are one sentence question no clear punctuation exist\n",
    "'''\n",
    "# NORMALIZATION\n",
    "normalized_sentence = pre_text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94bf7297650539b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-29T16:26:44.196520Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "# INITIAL tokenization we may need better implementations\n",
    "vocab, tokenized_sentence = tokenization(normalized_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cfb98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = snow_ball_stemmer(vocab)\n",
    "tokenized_sentence = snow_ball_stemmer(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee32218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_as_series = pd.Series(list(vocab))\n",
    "vocab_as_series.to_csv(\"vocab.csv\",index=False)\n",
    "tokenized_sentence.to_csv(\"tokenized_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbe9a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenized_sentence\n",
    "del vocab_as_series\n",
    "del normalized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad6a6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "del decoupled_structured_sentence\n",
    "del parsed_tree\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d32081f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_sentence = structured_sentence[\"train.TOP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d1f83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_orders, drink_orders, none_words = extract_pizza_drinks(structured_sentence.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3acacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_words = none_words.drop_duplicates()\n",
    "none_words = pre_text_normalization(none_words)\n",
    "none_words = none_words.reset_index(drop=True)\n",
    "vocab, none_words = tokenization(none_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cdea89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = snow_ball_stemmer(vocab)\n",
    "none_words = snow_ball_stemmer(none_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d9d83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_nodes, drink_nodes = extract_nodes(pizza_orders,drink_orders)\n",
    "pizza_nodes, drink_nodes = clean_extracted_nodes(pizza_nodes, drink_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3d06c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfold the list\n",
    "number, size, none, topping , quantity, style = pizza_nodes\n",
    "drink_type, container_type, volume = drink_nodes\n",
    "del pizza_nodes\n",
    "del drink_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b13c62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [number, size, none, topping, quantity, style, drink_type, container_type, volume]\n",
    "csv_file_names = [\"number\", \"size\", \"none\",\"topping\",\"quantity\",\"style\",\"drink_type\",\"container_type\",\"volume\"]\n",
    "none_vocab = vocab\n",
    "for label, csv in zip(labels, csv_file_names):\n",
    "    vocab, tokens = tokenization(label)\n",
    "    vocab = snow_ball_stemmer(vocab)\n",
    "    if csv == \"none\":\n",
    "        vocab.update(none_vocab)\n",
    "    vocab = pd.Series(list(vocab))\n",
    "    vocab.to_csv(f\"./labels/{csv}.csv\", index=False)\n",
    "    tokens.to_csv(f\"./tokens/{csv}_tokens.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b495dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
