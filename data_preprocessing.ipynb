{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.945306Z",
     "start_time": "2024-11-29T16:16:11.593866Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525e1e8c84335bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.951944Z",
     "start_time": "2024-11-29T16:16:15.947321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d825ea706bbc1c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.960799Z",
     "start_time": "2024-11-29T16:16:15.953961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Dataset given path to JSON file\n",
    "# input: JSON file   -> output: list of size 4 (sentence, EXR, TOP, TOP_DECOUPLED) * number of strings\n",
    "# we will use this to read the Training/ evaluation / test datasets. \n",
    "def read_dataset(path: str):\n",
    "    data = pd.read_json(path, lines = True)\n",
    "    columns = data.columns.tolist()\n",
    "    parsed_json = [None]*len(columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        parsed_json[i] = data[columns[i]] # IDK will it be easier to us to work with pandas or numpy\n",
    "    return parsed_json # we store data in list of PD.Series for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9974825debeb08f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:16:15.969130Z",
     "start_time": "2024-11-29T16:16:15.962816Z"
    }
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SERIES of String sentences\n",
    "def text_normalization(sentences):\n",
    "# convert words to lower\n",
    "    sentences = sentences.str.lower()\n",
    "# sizes are flexible some are party-sized, party size , lunch - sized ....\n",
    "# i will assume one format A-B : party-sized\n",
    "# now we build our regex where we search for \"(some shit) size\" and convert it to some shit-size \n",
    "    pattern_size =r'''(?xm) # multiline verbose flag to enable comments\n",
    "        \\b([a-z]*)[-\\s]*(size)(?:d)?\\b # i search for alphabets that is followed by one or more spaces or \"-\" then one or more spaces\n",
    "                                       # then size, not capturing d because i don't want it anymore (i don't want it in group actually)\n",
    "        '''\n",
    "    sentences = sentences.str.replace(pattern_size, r\"\\1-\\2\",regex=True)\n",
    "# sometimes they ask the pizza to be gluten-free, gluten free -> normalizing this to gluten-free\n",
    "# gluten-free is a complete style so i don't think that this will be removed\n",
    "    pattern_free = r'''(?xm) \n",
    "        \\b([a-z]*)[-\\s]*(free)\\b\n",
    "        '''\n",
    "    sentences = sentences.str.replace(pattern_free, r\"\\1-\\2\",regex=True)\n",
    "# sometimes they refer to pizza as pie : WE ONLY SELL PIZZA\n",
    "    sentences = sentences.str.replace(\"pie\", \"pizza\")\n",
    "# Quantities\n",
    "\n",
    "# there are alot of Quantitative items 3 pies, 250 ml, 552 ....\n",
    "# we can normalizing this to a quantity flag something like <Q>\n",
    "    pattern_digits = r\"[0-9]+\"\n",
    "# we make the flag as lowercase \"quta\" short for quantity \n",
    "    sentences = sentences.str.replace(pattern_digits, \"quta\", regex=True)    \n",
    "    '''\n",
    "    Now i want to take into consider quantities that means less of topping\n",
    "    something like not much not many .... we convert all those to a standard quantity : light\n",
    "    little bit of, not much, not many, just a bit, just a little, tiny bit\n",
    "    '''\n",
    "    less_quantities = [r\"\\blittle\\sbit\\sof\\b\",r\"\\bnot\\sm(?:uch|any)\\b\",r\"\\bjust\\sa\\s(bit|little)\\b\",r\"\\btiny\\sbit\\b\"]\n",
    "    for word in less_quantities:\n",
    "        sentences = sentences.str.replace(word,\"light\",regex=True)\n",
    "    '''\n",
    "    for quantities that mean much:\n",
    "    a lot of.\n",
    "    '''\n",
    "    more_quantities =[r\"\\ba\\slot\\sof\\b\"] # any extra that doesn't have large after\n",
    "    for word in more_quantities:\n",
    "        sentences = sentences.str.replace(word,\"extra\",regex=True)\n",
    "# Quantity like \"a\" pizza should be converted to \"one\"\n",
    "    sentences = sentences.str.replace(r\"\\ba\\b\",\"one\",regex=True)\n",
    "# Negation\n",
    "    '''\n",
    "    There is multiple ways of negation, what i found while searching:\n",
    "    Without, hold the, With no(t), no, avoid\n",
    "    i want complex words like (hold the , without) to be converted int no\n",
    "    i will use no to negate the whole toppings in the tokenizer\n",
    "    '''\n",
    "    negation_words = [r\"\\bwithout\\b\", r\"\\bhold\\sthe\\b\", r\"\\bavoid\\b\"]\n",
    "    for word in negation_words:\n",
    "        sentences = sentences.str.replace(word, \"no\" ,regex=True)\n",
    "\n",
    "\n",
    "# we may add more Normalization Techniques or delete some \"WHO KNOWS\" \n",
    "#TO DO: (I think BBQ topping needs to be paired with things, it's always written as bbq_chicken, bbq_sauce, bbq_pulled_pork... but i don't know for now)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1c145790e9de15",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# the function takes SERIES of string sentences -> outputs SET of vocab and , SERIES of list of tokens\n",
    "def tokenization(sentences):\n",
    "    # merge the whole series int one sentence to make the vocab extracting faster\n",
    "    all_words = ' '.join(sentences)\n",
    "    # split on \\s\n",
    "    all_words = all_words.split(r\" \")\n",
    "    # keep the unique \n",
    "    vocab = set(all_words)\n",
    "    # i want to tokenize things like I'd to I , 'd\n",
    "    # new tokens that will come from I'd, it's ,....\n",
    "    new_tokens = set()\n",
    "    for word in vocab:\n",
    "        temp2 = word.split(\"'\")\n",
    "        # to make sure the 2nd token has its apostrophe: 'd (it should be two splitted words)\n",
    "        temp2[-1] = \"'\"+temp2[-1]\n",
    "        new_tokens.update(temp2)\n",
    "    vocab.update(new_tokens)\n",
    "    # we use expand to split the series into Dataframe (I think this will accelerate when i try to map the word into other thing)\n",
    "    pattern = r\"\\b([a-z]*)'([a-z]*)\\b\"\n",
    "    sentences = sentences.str.replace(pattern, r\"\\1 '\\2\",regex=True)\n",
    "    sentences = sentences.str.split(\" \",expand=True)\n",
    "    sentences.fillna(\"\",inplace=True)\n",
    "   # negation check regex : \\b(?<=not?)(.*?)(?=(\\.|,|$|and))\\b (for the future maybe ?)\n",
    "    return vocab, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665a93f040d825db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:57.812078Z",
     "start_time": "2024-11-29T16:18:23.954568Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences, parsed_tree, structured_sentence, decoupled_structured_sentence = read_dataset(\"./PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbfa8b23bbfc2bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:19:45.717518Z",
     "start_time": "2024-11-29T16:18:57.815096Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "now we talk about the data preprocessing\n",
    "What we should take into consideration? \n",
    "1- Word Normalization\n",
    "2- Word Tokenization\n",
    "Why we won't use Sentence segmentation? It's useless, orders are one sentence question no clear punctuation exist\n",
    "'''\n",
    "# NORMALIZATION\n",
    "normalized_sentence = text_normalization(sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94bf7297650539b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-29T16:26:44.196520Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "# INITIAL tokenization we may need better implementations\n",
    "vocab, tokenized_sentence = tokenization(normalized_sentence.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33022b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_tree = pd.read_csv(\"EXR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d1f83a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIZZAORDER (NUMBER 1 ) (SIZE LARGE ) (TOPPING BBQ_PULLED_PORK ) )\n",
      "['NUMBER 1 ', 'SIZE LARGE ', 'TOPPING BBQ_PULLED_PORK ']\n"
     ]
    }
   ],
   "source": [
    "# in order to extract every node i will walk on string and match every (NODE)\n",
    "# problem with that is regex won't know what is the starting and ending of parenthesis \n",
    "# so i will extract ORDER from (ORDER(PIZZAORDER () )(DRINKORDER () )) then DRINKORDER then PIZZAORDER\n",
    "# this way DRINKORDER sees only 2 parenthesis (DRINKORDER .*) and after this part is deleted PIZZAORDER won't have a problem\n",
    "# thus i can just match every 2 parenthesis (.*) \n",
    "order, pizza_order, drink_order, nodes_list = None, None, None, []\n",
    "order_pattern = r\"(?<=\\(ORDER).*(?=\\))\"\n",
    "drink_order_pattern = r\"(\\(DRINKORDER).*(?=\\))\"\n",
    "node_pattern = r\"(?<=\\().*?(?=\\))\"\n",
    "\n",
    "x= \"(ORDER (PIZZAORDER (NUMBER 1 ) (SIZE LARGE ) (TOPPING BBQ_PULLED_PORK ) )(DRINKORDER (NUMBER 2)))\"\n",
    "order = re.search(re.compile(order_pattern),x).group()\n",
    "order = order.strip()\n",
    "drink_order = re.search(re.compile(drink_order_pattern),order)\n",
    "if(drink_order != None):\n",
    "    pizza_order = order[1:drink_order.span()[0]]\n",
    "    drink_order = drink_order.group()\n",
    "    print(pizza_order)\n",
    "else:\n",
    "    pizza_order = order\n",
    "nodes = re.finditer(re.compile(node_pattern), pizza_order)\n",
    "if nodes != None:\n",
    "    nodes_list.extend([x.group() for x in nodes]) # will get changed\n",
    "print(nodes_list)\n",
    "# nodes can be :\n",
    "# for PIZZAORDER: NUMBER, SIZE, TOPPING, COMPLEX_TOPPING, NOT\n",
    "# for DRINKORDER: DRINKTYPE, CONTAINERTYPE, VOLUME\n",
    "# tomorrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef928e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
